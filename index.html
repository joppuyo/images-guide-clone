<!doctype html><html lang="en"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="description" content="A free book on modern image optimization techniques. Formats, decoders, techniques for efficient compression and more are covered. Author: Addy Osmani"><meta name="keywords" content="compress images, image optimization, compression, smaller images, shrink images, compress jpeg, faster images, compress png, compress gif, lazy-load images, faster image loading, reduce image size, make my website faster"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Essential Image Optimization</title><link rel="preload" href="scripts/main.min.js" as="script"><link rel="preconnect" href="https://res.cloudinary.com"><link rel="preconnect" href="https://www.google-analytics.com"><meta name="msapplication-tap-highlight" content="no"><link rel="apple-touch-icon" sizes="180x180" href="images/touch/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="images/touch/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="images/touch/favicon-16x16.png"><link rel="manifest" href="manifest.json"><link rel="mask-icon" href="images/touch/safari-pinned-tab.svg" color="#009aff"><link rel="shortcut icon" href="images/touch/favicon.ico"><meta name="msapplication-config" content="images/touch/browserconfig.xml"><meta name="theme-color" content="#ffffff"><style type="text/css">
    body,html{font:1.1em/1.618 'avenir next',avenir,sans-serif}body{margin:0}header{background:#00c6ff;background:-webkit-linear-gradient(to right,#0072ff,#00c6ff);background:linear-gradient(to right,#0072ff,#00c6ff);height:44vh;display:flex;flex-direction:column;align-items:center;justify-content:center}header .logo{max-width:130px;padding:30px 0}.author{color:#fff}.author a{color:#fff;font-weight:600;text-decoration:none;border-bottom:3px solid transparent}.reviewers{color:#f1ece9}.reviewers a{color:#fff;font-weight:600;text-decoration:none;border-bottom:3px solid transparent}.container{word-wrap:break-word}.container{margin:0 auto;max-width:90%;padding-top:16px}.author{padding:0 16px;margin:0 auto 20px;font-size:1.1em}.reviewers{padding:0 16px;margin:0 auto 14px;font-size:.7em}header{padding:40px 0}header .credits{font-size:.75em;padding:0 16px;text-align:center;max-width:84%}@media (max-height:320px){.credits{display:none}}@media (min-width:641px){body,html{font-size:1.15em;line-height:1.618}header .logo{max-width:100px}header{padding:60px 0}header .credits{font-size:1em;line-height:normal;padding:0;max-width:60%}.author{margin:0 auto 30px;font-size:.8em}.reviewers{font-size:.5em}.github-corner{right:0}.container{margin-top:65px;max-width:100%;padding-top:0}img{max-width:90%}img{width:90%;max-width:90%}}@media (min-width:768px){header .logo{max-width:200px;padding:40px 0}.author{font-size:1.1em}.reviewers{font-size:.7em}}@media (min-width:961px){body,html{font-size:1.2em;line-height:1.618}}@media (min-width:1025px){body,html{font-size:1.225em;line-height:1.618}}.github-corner{position:absolute;top:0;right:0;height:5rem;width:5rem;fill:#00c3ff}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}
</style>
<link rel="preload" href="styles/main.css" as="style" onload="this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="styles/main.css"></noscript><meta itemprop="name" content="Essential Image Optimization"><meta itemprop="description" content="A free book on modern image optimization techniques. Formats, decoders, techniques for efficient compression and more are covered."><meta itemprop="image" content="https://images.guide/images/logo-banner.jpg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@addyosmani"><meta name="twitter:title" content="Essential Image Optimization"><meta name="twitter:description" content="A free book on modern image optimization techniques. Formats, decoders, techniques for efficient compression and more are covered."><meta name="twitter:creator" content="@addyosmani"><meta name="twitter:image" content="https://images.guide/images/logo-banner.jpg"><meta name="twitter:image:alt" content="A free book on modern image optimization techniques"><meta property="og:title" content="summary_large_image"><meta property="og:type" content="website"><meta property="og:url" content="https://images.guide/"><meta property="og:image" content="https://images.guide/images/logo-banner.jpg"><meta property="og:description" content="A free book on modern image optimization techniques. Formats, decoders, techniques for efficient compression and more are covered."><meta property="fb:admins" content="129712729600"><header><img alt="Essential Image Optimization" class="logo" src="/images/icons/logo.svg"><div class="credits"><div class="author">An eBook by <a href="https://twitter.com/addyosmani">Addy Osmani</a></div><div class="reviewers">With thanks to: <a href="https://twitter.com/kornelski">Kornel Lesi&#x144;ski<a>, <a href="https://twitter.com/estellevw">Estelle Weyl</a>, <a href="https://twitter.com/malchata">Jeremy Wagner</a>, <a href="https://twitter.com/tkadlec">Tim Kadlec</a>, <a href="https://twitter.com/NolanOBrien">Nolan O’Brien</a>, <a href="https://twitter.com/patmeenan">Pat Meenan</a>, <a href="https://twitter.com/kristoferbaxter">Kristofer Baxter</a>, <a href="https://twitter.com/HenriHelvetica">Henri Helvetica</a>, <a href="https://twitter.com/hdjirdeh">Houssein Djirdeh</a>, <a href="https://twitter.com/una">Una Kravets</a>, <a href="https://twitter.com/ARebelBelle">Elle Osmani</a> and <a href="https://twitter.com/igrigorik">Ilya Grigorik</a> for their help and reviews.</a></a></div></div><a href="https://github.com/addyosmani/essential-image-optimisation" class="github-corner link" aria-label="View source on GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 250 250"><path d="M0 0l115 115h15l12 27 108 108V0z" fill="#fff"/><path class="octo-arm" d="M128 109c-15-9-9-19-9-19 3-7 2-11 2-11-1-7 3-2 3-2 4 5 2 11 2 11-3 10 5 15 9 16" style="-webkit-transform-origin: 130px 106px; transform-origin: 130px 106px"/><path class="octo-body" d="M115 115s4 2 5 0l14-14c3-2 6-3 8-3-8-11-15-24 2-41 5-5 10-7 16-7 1-2 3-7 12-11 0 0 5 3 7 16 4 2 8 5 12 9s7 8 9 12c14 3 17 7 17 7-4 8-9 11-11 11 0 6-2 11-7 16-16 16-30 10-41 2 0 3-1 7-5 11l-12 11c-1 1 1 5 1 5z"/></svg></a></header><div class="container"><noscript><p>You are viewing this book without JavaScript enabled. Images will be loaded immediately rather than lazy-loaded.</p></noscript><h3 id="-a-id-the-tldr-href-the-tldr-the-tl-dr-a-"><a id="the-tldr" href="#the-tldr">The tl;dr</a></h3><p><strong>We should all be automating our image compression.</strong></p><p>Image optimization should be automated. It’s easy to forget, best practices change, and content that doesn’t go through a build pipeline can easily slip. To automate: Use <a href="https://github.com/imagemin/imagemin">imagemin</a> or <a href="https://github.com/jcupitt/libvips">libvips</a> for your build process. Many alternatives exist.</p><p>Most CDNs (e.g. <a href="https://www.akamai.com/us/en/solutions/why-akamai/image-management.jsp">Akamai</a>) and third-party solutions like <a href="https://cloudinary.com">Cloudinary</a>, <a href="https://imgix.com">imgix</a>, <a href="https://www.fastly.com/io/">Fastly’s Image Optimizer</a>, <a href="https://www.instartlogic.com/technology/machine-learning/smartvision">Instart Logic’s SmartVision</a> or <a href="https://imageoptim.com/api">ImageOptim API</a> offer comprehensive automated image optimization solutions.</p><p>The amount of time you’ll spend reading blog posts and tweaking your configuration is greater than the monthly fee for a service (Cloudinary has a <a href="http://cloudinary.com/pricing">free</a> tier). If you don’t want to outsource this work for cost or latency concerns, the open-source options above are solid. Projects like <a href="https://github.com/imazen/imageflow">Imageflow</a> or <a href="https://github.com/thumbor/thumbor">Thumbor</a> enable self-hosted alternatives.</p><p><strong>Everyone should be compressing their images efficiently.</strong></p><p>At minimum: use <a href="https://imageoptim.com/">ImageOptim</a>. It can significantly reduce the size of images while preserving visual quality. Windows and Linux <a href="https://imageoptim.com/versions.html">alternatives</a> are also available.</p><p>More specifically: run your JPEGs through <a href="https://github.com/mozilla/mozjpeg">MozJPEG</a> (<code>q=80</code> or lower is fine for web content) and consider <a href="http://cloudinary.com/blog/progressive_jpegs_and_green_martians">Progressive JPEG</a> support, PNGs through <a href="https://pngquant.org/">pngquant</a> and SVGs through <a href="https://github.com/svg/svgo">SVGO</a>. Explicitly strip out metadata (<code>--strip</code> for pngquant) to avoid bloat. Instead of crazy huge animated GIFs, deliver <a href="https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC">H.264</a> videos (or <a href="https://www.webmproject.org/">WebM</a> for Chrome, Firefox and Opera)! If you can’t at least use <a href="https://github.com/pornel/giflossy">Giflossy</a>. If you can spare the extra CPU cycles, need higher-than-web-average quality and are okay with slow encode times: try <a href="https://research.googleblog.com/2017/03/announcing-guetzli-new-open-source-jpeg.html">Guetzli</a>.</p><p>Some browsers advertise support for image formats via the Accept request header. This can be used to conditionally serve formats: e.g. lossy <a href="https://developers.google.com/speed/webp/">WebP</a> for Blink-based browsers like Chrome and fallbacks like JPEG/PNG for other browsers.</p><p>There’s always more you can do. Tools exists to generate and serve <code>srcset</code> breakpoints. Resource selection can be automated in Blink-based browsers with <a href="https://developers.google.com/web/updates/2015/09/automating-resource-selection-with-client-hints">client-hints</a> and you can ship fewer bytes to users who opted into ‘data savings’ in-browser by heeding the <a href="https://developers.google.com/web/updates/2016/02/save-data">Save-Data</a> hint.</p><p>The smaller in file-size you can make your images, the better a network experience you can offer your users – especially on mobile. In this write-up, we’ll look at ways to reduce image size through modern compression techniques with minimal impact to quality.<details><summary><h2>Table of Contents</h2></summary><p><ul><li><a href="#introduction">Introduction</a></li><li><a href="#do-my-images-need-optimization">How can I tell if my images need to be optimized?</a></li><li><a href="#choosing-an-image-format">How do I choose an image format?</a></li><li><a href="#the-humble-jpeg">The humble JPEG</a></li><li><a href="#jpeg-compression-modes">JPEG compression modes</a><ul><li><a href="#the-advantages-of-progressive-jpegs">The advantages of Progressive JPEGs</a></li><li><a href="#whos-using-progressive-jpegs-in-production">Who’s using Progressive JPEGs in production?</a></li><li><a href="#the-disadvantages-of-progressive-jpegs">The disadvantages of Progressive JPEGs</a></li><li><a href="#how-to-create-progressive-jpegs">How do you create Progressive JPEGs?</a></li><li><a href="#chroma-subsampling">Chroma (or Color) Subsampling</a></li><li><a href="#how-far-have-we-come-from-the-jpeg">How far have we come from the JPEG?</a></li><li><a href="#optimizing-jpeg-encoders">Optimizing JPEG Encoders</a></li><li><a href="#what-is-mozjpeg">What is MozJPEG?</a></li><li><a href="#what-is-guetzli">What is Guetzli?</a></li><li><a href="#mozjpeg-vs-guetzli">How does MozJPEG compare to Guetzli?</a></li><li><a href="#butteraugli">Butteraugli</a></li></ul></li><li><a href="#what-is-webp">What is WebP?</a><ul><li><a href="#how-does-webp-perform">How does WebP perform?</a></li><li><a href="#whos-using-webp-in-production">Who’s using WebP in production?</a></li><li><a href="#how-does-webp-encoding-work">How does WebP encoding work?</a></li><li><a href="#webp-browser-support">WebP browser support</a></li><li><a href="#how-do-i-convert-to-webp">How do I convert my images to WebP?</a></li><li><a href="#how-do-i-view-webp-on-my-os">How do I view WebP images on my OS?</a></li><li><a href="#how-do-i-serve-webp">How do I serve WebP?</a></li></ul></li><li><a href="#svg-optimization">SVG optimization</a></li><li><a href="#avoid-recompressing-images-lossy-codecs">Avoid recompressing images with lossy codecs</a></li><li><a href="#reduce-unnecessary-image-decode-costs">Reduce unnecessary image decode and resize costs</a><ul><li><a href="#delivering-hidpi-with-srcset">Delivering HiDPI images using <code>srcset</code></a></li><li><a href="#art-direction">Art direction</a></li></ul></li><li><a href="#color-management">Color management</a></li><li><a href="#image-sprites">Image spriting</a></li><li><a href="#lazy-load-non-critical-images">Lazy-load non-critical images</a></li><li><a href="#display-none-trap">Avoiding the <code>display: none;</code> trap</a></li><li><a href="#image-processing-cdns">Does an image processing CDN make sense for you?</a></li><li><a href="#caching-image-assets">Caching image assets</a></li><li><a href="#preload-critical-image-assets">Preloading critical image assets</a></li><li><a href="#performance-budgets">Performance Budgets For Images</a></li><li><a href="#closing-recommendations">Closing recommendations</a></li><li><a href="#trivia">Trivia</a></li></ul></p></details></p><h3 id="-a-id-introduction-href-introduction-introduction-a-"><a id="introduction" href="#introduction">Introduction</a></h3><p><strong>Images are still the number one cause of bloat on the web.</strong></p><p>Images take up massive amounts of internet bandwidth because they often have large file sizes. According to the <a href="http://httparchive.org/">HTTP Archive</a>, 60% of the data transferred to fetch a web page is images composed of JPEGs, PNGs and GIFs. As of July 2017, images accounted for <a href="http://httparchive.org/interesting.php#bytesperpage">1.7MB</a> of the content loaded for the 3.0MB average site.</p><p>Per Tammy Everts, adding images to a page or making existing images larger have been <a href="https://calendar.perfplanet.com/2014/images-are-king-an-image-optimization-checklist-for-everyone-in-your-organization/">proven</a> to increase conversion rates. It’s unlikely that images will go away and so investing in an efficient compression strategy to minimize bloat becomes important.<figure><picture><source data-srcset="images/book-images/Modern-Image00-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image00-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image00-large.jpg"><img class="lazyload small" data-src="images/book-images/Modern-Image00-large.jpg" alt="Fewer images per page create more conversions. 19 images per page on average converted better than 31 images per page on average."><noscript><img src="images/book-images/Modern-Image00-small.jpg"></noscript></picture><figcaption>Per <a href="https://www.thinkwithgoogle.com/marketing-resources/experience-design/mobile-page-speed-load-time/">Soasta/Google research</a> from 2016, images were the 2nd highest predictor of conversions with the best pages having 38% fewer images.</figcaption></figure></p><p>Image optimization consists of different measures that can reduce the file size of your images. It ultimately depends on what visual fidelity your images require.<figure><picture><source data-srcset="images/book-images/image-optimisation-small.jpeg" media="(max-width: 640px)"><source data-srcset="images/book-images/image-optimisation-medium.jpeg" media="(max-width: 1024px)"><source data-srcset="images/book-images/image-optimisation-large.jpeg"><img class="lazyload small" data-src="images/book-images/image-optimisation-large.jpeg" alt="Image optimization covers a number of different techniques"><noscript><img src="images/book-images/image-optimisation-large.jpeg"></noscript></picture><figcaption><strong>Image optimization:</strong> Choose the right format, compress carefully and prioritize critical images over those that can be lazy-loaded.</figcaption></figure></p><p>Common image optimizations include compression, responsively serving them down based on screen size using <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/picture"><code>&lt;picture&gt;</code></a>/<a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images"><code>&lt;img srcset&gt;</code></a>, and resizing them to reduce image decode costs.<figure><picture><source data-srcset="images/book-images/chart_naedwl-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/chart_naedwl-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/chart_naedwl-large.jpg"><img class="lazyload small" data-src="images/book-images/chart_naedwl-large.jpg" alt="A histogram of potential image savings from the HTTP Archive validating the 30KB of potential image savings at the 95th percentile."><noscript><img src="images/book-images/chart_naedwl-large.jpg"></noscript></picture><figcaption>Per the <a href="http://jsfiddle.net/rviscomi/rzneberp/embedded/result/">HTTP Archive</a>, per-image savings at the 95th percentile (looking at the Cumulative Distribution Function) are 30KB!</figcaption></figure></p><p>There’s plenty of room for us to collectively optimize images better.<figure><picture><source data-srcset="images/book-images/image-optim-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/image-optim-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/image-optim-large.jpg"><img class="lazyload small" data-src="images/book-images/image-optim-large.jpg" alt="ImageOptim in use on Mac with a number of images that have been compressed with savings over 50%"><noscript><img src="images/book-images/image-optim-large.jpg"></noscript></picture><figcaption>ImageOptim is free, reduces image size through modern compression techniques and by stripping unnecessary EXIF meta-data.</figcaption></figure></p><p>If you’re a designer, there’s also an <a href="https://github.com/ImageOptim/Sketch-plugin">ImageOptim plugin for Sketch</a> that will optimize your assets on export. I’ve found it a huge time saver.</p><h3 id="-a-id-do-my-images-need-optimization-href-do-my-images-need-optimization-how-can-i-tell-if-my-images-need-to-be-optimized-a-"><a id="do-my-images-need-optimization" href="#do-my-images-need-optimization">How can I tell if my images need to be optimized?</a></h3><p>Perform a site audit through <a href="https://www.webpagetest.org/">WebPageTest.org</a> and it will highlight opportunities to better optimize your images (see ‘Compress Images’).<figure><picture><source data-srcset="images/book-images/Modern-Image1-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image1-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image1-large.jpg"><img class="lazyload small" data-src="images/book-images/Modern-Image1-large.jpg" alt="WebPage test supports auditing for image compression via the compress images section"><noscript><img src="images/book-images/Modern-Image1-large.jpg"></noscript></picture><figcaption>The ‘Compress Images’ section of a WebPageTest report lists images that can be compressed more efficiently and the estimated file-size savings of doing so.</figcaption></figure><figure><picture><source data-srcset="images/book-images/Modern-Image2-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image2-large.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image2-medium.jpg"><img class="lazyload small" data-src="images/book-images/Modern-Image2-medium.jpg" alt="image compression recommendations from webpagetest"><noscript><img src="images/book-images/Modern-Image2-medium.jpg"></noscript></picture></figure></p><p><a href="https://developers.google.com/web/tools/lighthouse/">Lighthouse</a> audits for performance best practices. It includes audits for image optimisation and can make suggestions for images that could be compressed further or point out images that are off-screen and could be lazy-loaded.</p><p>As of Chrome 60, Lighthouse now powers the <a href="https://developers.google.com/web/updates/2017/05/devtools-release-notes#lighthouse">Audits panel</a> in the Chrome DevTools:<figure><picture><source data-srcset="images/book-images/hbo-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/hbo-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/hbo-large.jpg"><img class="lazyload small" data-src="images/book-images/hbo-large.jpg" alt="Lighthouse audit for HBO.com, displaying image optimisation recommendations"><noscript><img src="images/book-images/hbo-large.jpg"></noscript></picture><figcaption>Lighthouse can audit for Web Performance, Best Practices and Progressive Web App features.</figcaption></figure></p><p>You may also be familiar of other performance auditing tools like <a href="https://developers.google.com/speed/pagespeed/insights/">PageSpeed Insights</a> or <a href="https://webspeedtest.cloudinary.com/">Website Speed Test</a> by Cloudinary which includes a detailed image analysis audit.</p><h2 id="-a-id-choosing-an-image-format-href-choosing-an-image-format-how-do-i-choose-an-image-format-a-"><a id="choosing-an-image-format" href="#choosing-an-image-format">How do I choose an image format?</a></h2><p>As Ilya Grigorik notes in his excellent <a href="https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/image-optimization">image optimization guide</a>, the ‘right format’ for an image is a combination of desired visual results and functional requirements. Are you working with Raster or Vector images?<figure><picture><source data-srcset="images/book-images/rastervvector-medium.png" media="(max-width: 640px)"><source data-srcset="images/book-images/rastervvector-large.png" media="(max-width: 1024px)"><source data-srcset="images/book-images/rastervvector-small.png"><img class="lazyload very-small" data-src="images/book-images/rastervvector-small.png" alt="vector vs raster images"><noscript><img src="images/book-images/rastervvector-small.png"></noscript></picture></figure></p><p><a href="https://en.wikipedia.org/wiki/Raster_graphics">Raster graphics</a> represent images by encoding the values of each pixel within a rectangular grid of pixels. They are not resolution or zoom independent. WebP or widely supported formats like JPEG or PNG handle these graphics well where photorealism is a necessity. Guetzli, MozJPEG and other ideas we’ve discussed apply well to raster graphics.</p><p><a href="https://en.wikipedia.org/wiki/Vector_graphics">Vector graphics</a> use points, lines and polygons to represent images and formats using simple geometric shapes (e.g. logos) offering a high-resolution and zoom like SVG handle this use case better.</p><p>The wrong format can cost you. The logical flow for choosing the right format can be fraught with peril so experiment with the savings other formats can afford with care.</p><p>Jeremy Wagner has covered <a href="http://jlwagner.net/talks/these-images/#/2/2">trade-offs</a> worth considering when evaluating formats in his image optimization talks.</p><h2 id="-a-id-the-humble-jpeg-href-the-humble-jpeg-the-humble-jpeg-a-"><a id="the-humble-jpeg" href="#the-humble-jpeg">The humble JPEG.</a></h2><p>The <a href="https://en.wikipedia.org/wiki/JPEG">JPEG</a> may well be the world’s most widely used image format. As noted earlier, <a href="http://httparchive.org/interesting.php">45% of the images</a> seen on sites crawled by HTTP Archive are JPEGs. Your phone, your digital SLR, that old webcam – everything pretty much supports this codec. It’s also very old, dating all the way back to 1992 when it was first released. In that time, there’s been an immense body of research done attempting to improve what it offers.</p><p>JPEG is a lossy compression algorithm that discards information in order to save space and many of the efforts that came after it attempted to preserve visual fidelity while keeping file sizes as small as possible.</p><p><strong>What image quality is acceptable for your use-case?</strong></p><p>Formats like JPEG are best suited for photographs or images with a number of color regions. Most optimisation tools will allow you to set what level of compression you’re happy with; higher compression reduces file size but can introduce artifacts, halos or blocky degrading.<figure><picture><source data-srcset="images/book-images/Modern-Image5-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image5-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image5-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image5-large.jpg" alt="JPEG compression artifacts can be increasingly perceived as we shift from best quality to lowest"><noscript><img src="images/book-images/Modern-Image5-large.jpg"></noscript></picture><figcaption>JPEG: Perceivable JPEG compression artifacts can increase as we shift from best quality to lowest. Note that image quality scores in one tool can be very different to quality scores in another.</figcaption></figure></p><p>When choosing what quality setting to opt for, consider what quality bucket your images fall into:<ul><li><strong>Best quality</strong> – when quality matters more than bandwidth. This may be because the image has high prominence in your design or is displayed at full resolution.</li><li><strong>Good quality</strong> – when you care about shipping smaller file-sizes, but don’t want to negatively impact image quality too much. Users still care about some level of image quality.</li><li><strong>Low quality</strong> – when you care enough about bandwidth that image degradation is okay. These images are suitable for spotty/poor network conditions.</li><li><strong>Lowest quality</strong> – bandwidth savings are paramount. Users want a decent experience but will accept a pretty degraded experience for the benefit of pages loading more quickly.</li></ul></p><p>Next, let’s talk about JPEG’s compression modes as these can have a large impact on perceived performance.<aside class="note"><b>Note:</b> It’s possible that we sometimes overestimate the image quality that our users need. Image quality could be considered a deviation from an ideal, uncompressed source. It can also be subjective.</aside></p><h2 id="-a-id-jpeg-compression-modes-href-jpeg-compression-modes-jpeg-compression-modes-a-"><a id="jpeg-compression-modes" href="#jpeg-compression-modes">JPEG compression modes</a></h2><p>The JPEG image format has a number of different <a href="http://cs.haifa.ac.il/~nimrod/Compression/JPEG/J5mods2007.pdf">compression modes</a>. Three popular modes are baseline (sequential), Progressive JPEG (PJPEG) and lossless.</p><p><strong>How do baseline (or sequential) JPEGs and Progressive JPEGs differ?</strong></p><p>Baseline JPEGs (the default for most image editing and optimisation tools) are encoded and decoded in a relatively simple manner: top to bottom. When baseline JPEGs load on slow or spotty connections, users see the top of the image with more of it revealed as the image loads. Lossless JPEGs are similar but have a smaller compression ratio.<figure><picture><source data-srcset="images/book-images/Modern-Image6-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image6-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image6-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image6-large.jpg" alt="baseline JPEGs load top to bottom"><noscript><img src="images/book-images/Modern-Image6-large.jpg"></noscript></picture><figcaption>Baseline JPEGs load top to bottom while Progressive JPEGs load from blurry to sharp.</figcaption></figure></p><p>Progressive JPEGs divide the image into a number of scans. The first scan shows the image in a blurry or low-quality setting and following scans improve image quality. Think of this as ‘progressively’ refining it. Each ‘scan’ of an image adds an increasing level of detail. When combined this creates a full-quality image.<figure><picture><source data-srcset="images/book-images/Modern-Image7-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image7-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image7-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image7-large.jpg" alt="progressive JPEGs load from low-resolution to high-resolution"><noscript><img src="images/book-images/Modern-Image7-large.jpg"></noscript></picture><figcaption>Baseline JPEGs load images from top to bottom. PJPEGs load from low-resolution (blurry) to high-resolution. Pat Meenan wrote an <a href="http://www.patrickmeenan.com/progressive/view.php?img=https%3A%2F%2Fwww.nps.gov%2Fplanyourvisit%2Fimages%2FGrandCanyonSunset_960w.jpg">interactive tool</a> to test out and learn about Progressive JPEG scans too.</figcaption></figure></p><p>Lossless JPEG optimization can be achieved by <a href="http://www.verexif.com/en/">removing EXIF data</a> added by digital cameras or editors, optimizing an image’s <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman tables</a>, or rescanning the image. Tools like <a href="http://jpegclub.org/jpegtran/">jpegtran</a> achieve lossless compression by rearranging the compressed data without image degradation. <a href="https://github.com/kud/jpegrescan">jpegrescan</a>, <a href="https://github.com/tjko/jpegoptim">jpegoptim</a> and <a href="https://github.com/mozilla/mozjpeg">mozjpeg</a> (which we’ll cover shortly) also support lossless JPEG compression.</p><h3 id="-a-id-the-advantages-of-progressive-jpegs-href-the-advantages-of-progressive-jpegs-the-advantages-of-progressive-jpegs-a-"><a id="the-advantages-of-progressive-jpegs" href="#the-advantages-of-progressive-jpegs">The advantages of Progressive JPEGs</a></h3><p>The ability for PJPEGs to offer low-resolution ‘previews’ of an image as it loads improves perceived performance – users can feel like the image is loading faster compared to adaptive images.</p><p>On slower 3G connections, this allows users to see (roughly) what’s in an image when only part of the file has been received and make a call on whether to wait for it to fully load. This can be more pleasant than the top-to-bottom display of images offered by baseline JPEGs.<figure><picture><source data-srcset="images/book-images/pjpeg-graph-small.png" media="(max-width: 640px)"><source data-srcset="images/book-images/pjpeg-graph-medium.png" media="(max-width: 1024px)"><source data-srcset="images/book-images/pjpeg-graph-large.png"><img class="lazyload small" data-src="images/book-images/pjpeg-graph-large.png" alt="impact to wait time of switching to progressive jpeg"><noscript><img src="images/book-images/pjpeg-graph-large.png"></noscript></picture><figcaption>In 2015, <a href="https://code.facebook.com/posts/857662304298232/faster-photos-in-facebook-for-ios/">Facebook switched to PJPEG (for their iOS app)</a> and saw a 10% reduction in data usage. They were able to show a good quality image 15% faster than previously, optimising perceived loading time, as shown in the figure above.</figcaption></figure></p><p>PJPEGs can improve compression, consuming <a href="http://www.bookofspeed.com/chapter5.html">2-10%</a> less bandwidth compared to baseline/simple JPEGs for images over 10KB. Their higher compression ratio is thanks to each scan in the JPEG being able to have its own dedicated optional <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman table</a>. Modern JPEG encoders (e.g. <a href="http://libjpeg-turbo.virtualgl.org/">libjpeg-turbo</a>, MozJPEG, etc.) take advantage of PJPEG’s flexibility to pack data better.<aside class="note"><b>Note:</b> Why do PJPEGs compress better? Baseline JPEG blocks are encoded one at a time. With PJPEGs, similar <a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform">Discrete Cosine Transform</a> coefficients across more than one block can be encoded together leading to better compression.</aside></p><h3 id="-a-id-whos-using-progressive-jpegs-in-production-href-whos-using-progressive-jpegs-in-production-who-s-using-progressive-jpegs-in-production-a-"><a id="whos-using-progressive-jpegs-in-production" href="#whos-using-progressive-jpegs-in-production">Who’s using Progressive JPEGs in production?</a></h3><ul><li><a href="https://www.webpagetest.org/performance_optimization.php?test=170717_NQ_1K9P&amp;run=2#compress_images">Twitter.com ships Progressive JPEGs</a> with a baseline of quality of 85%. They measured user perceived latency (time to first scan and overall load time) and found overall, PJPEGs were competitive at addressing their requirements for low file-sizes, acceptable transcode and decode times.</li><li><a href="https://code.facebook.com/posts/857662304298232/faster-photos-in-facebook-for-ios/">Facebook ships Progressive JPEGs for their iOS app</a>. They found it reduced data usage by 10% and enabled them to show a good quality image 15% faster.</li><li><a href="https://engineeringblog.yelp.com/2017/06/making-photos-smaller.html">Yelp switched to Progressive JPEGs</a> and found it was in part responsible for ~4.5% of their image size reduction savings. They also saved an extra 13.8% using MozJPEG.</li></ul><p>Many other image-heavy sites, like <a href="https://pinterest.com">Pinterest</a> also use Progressive JPEGs in production.<figure><picture><source data-srcset="images/book-images/pinterest-loading-small.png" media="(max-width: 640px)"><source data-srcset="images/book-images/pinterest-loading-medium.png" media="(max-width: 1024px)"><source data-srcset="images/book-images/pinterest-loading-large.png"><img class="lazyload small" data-src="images/book-images/pinterest-loading-large.png" alt="Pinterests JPEGs are all progressively encoded. This optimizes the user experience by loading them each scan-by-scan."><noscript><img src="images/book-images/pinterest-loading-large.png"></noscript></picture><figcaption>Pinterest’s JPEGs are all progressively encoded. This optimizes the user experience by loading them each scan-by-scan.</figcaption></figure></p><h3 id="-a-id-the-disadvantages-of-progressive-jpegs-href-the-disadvantages-of-progressive-jpegs-the-disadvantages-of-progressive-jpegs-a-"><a id="the-disadvantages-of-progressive-jpegs" href="#the-disadvantages-of-progressive-jpegs">The disadvantages of Progressive JPEGs</a></h3><p>PJPEGs can be slower to decode than baseline JPEGs – sometimes taking 3× as long. On desktop machines with powerful CPUs this can be less of a concern, but is on underpowered mobile devices with limited resources. Displaying incomplete layers takes work as you’re basically decoding the image multiple times. These multiple passes can eat CPU cycles.</p><p>Progressive JPEGs are also not <em>always</em> smaller. For very small images (like thumbnails), progressive JPEGs can be larger than their baseline counterparts. However for such small thumbnails, progressive rendering might not really offer as much value.</p><p>This means that when deciding whether or not to ship PJPEGs, you’ll need to experiment and find the right balance of file-size, network latency and use of CPU cycles.</p><p>Note: PJPEGs (and all JPEGs) can sometimes be hardware decodable on mobile devices. It doesn’t improve on RAM impact, but it can negate some of the CPU concerns. Not all Android devices have hardware-acceleration support, but high end devices do, and so do all iOS devices.</p><p>Some users may consider progressive loading to be a disadvantage as it can become hard to tell when an image has completed loading. As this can vary heavily per audience, evaluate what makes sense for your own users.</p><h3 id="-a-id-how-to-create-progressive-jpegs-href-how-to-create-progressive-jpegs-how-do-you-create-progressive-jpegs-a-"><a id="how-to-create-progressive-jpegs" href="#how-to-create-progressive-jpegs">How do you create Progressive JPEGs?</a></h3><p>Tools and libraries like <a href="https://www.imagemagick.org/">ImageMagick</a>, <a href="http://libjpeg.sourceforge.net/">libjpeg</a>, <a href="http://jpegclub.org/jpegtran/">jpegtran</a>, <a href="https://github.com/danielgtaylor/jpeg-archive">jpeg-recompress</a> and <a href="https://github.com/imagemin/imagemin">imagemin</a> support exporting Progressive JPEGs. If you have an existing image optimization pipeline, there’s a good likelihood that adding progressive loading support could be straight-forward:<pre><code class="lang-js">const gulp = require(&#39;gulp&#39;);
const imagemin = require(&#39;gulp-imagemin&#39;);

gulp.task(&#39;images&#39;, function () {
    return gulp.src(&#39;images/*.jpg&#39;)
        .pipe(imagemin({
            progressive: true
        }))
        .pipe(gulp.dest(&#39;dist&#39;));       
});
</code></pre></p><p>Most image editing tools save images as Baseline JPEG files by default.<figure><picture><source data-srcset="images/book-images/photoshop-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/photoshop-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/photoshop-large.jpg"><img class="lazyload" data-src="images/book-images/photoshop-large.jpg" alt="photoshop supports exporting to progressive jpeg from the file export menu"><noscript><img src="images/book-images/photoshop-large.jpg"></noscript></picture><figcaption>Most image editing tools save images as Baseline JPEG files by default. You can save any image you create in Photoshop as a Progressive JPEG by going to File -&gt; Export -&gt; Save for Web (legacy) and then clicking on the Progressive option. Sketch also supports exporting Progressive JPEGs – export as JPG and check the ‘Progressive’ checkbox while saving your images.</figcaption></figure></p><h3 id="-a-id-chroma-subsampling-href-chroma-subsampling-chroma-or-color-subsampling-a-"><a id="chroma-subsampling" href="#chroma-subsampling">Chroma (or Color) Subsampling</a></h3><p>Our eyes are more forgiving to loss of color detail in an image (chroma) than they are luminance (or luma for short – a measure of brightness). <a href="https://en.wikipedia.org/wiki/Chroma_subsampling">Chroma subsampling</a> is a form of compression that reduces the precision of color in a signal in favor of luma. This reduces file size, in some cases by up to <a href="https://calendar.perfplanet.com/2015/why-arent-your-images-using-chroma-subsampling/">15-17%</a>, without adversely affecting image quality and is an option available for JPEG images. Subsampling can also reduce image memory usage.<figure><picture><source data-srcset="images/book-images/luma-signal-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/luma-signal-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/luma-signal-large.jpg"><img class="lazyload" data-src="images/book-images/luma-signal-large.jpg" alt="signal = chroma + luma"><noscript><img src="images/book-images/luma-signal-large.jpg"></noscript></picture></figure></p><p>As contrast is responsible for forming shapes that we see in an image, luma, which defines it, is pretty important. Older or filtered black and white photos may not contain color, but thanks to luma, they can be just as detailed as their color counterparts. Chroma (color) has less of an impact on visual perception.<figure><picture><source data-srcset="images/book-images/no-subsampling-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/no-subsampling-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/no-subsampling-large.jpg"><img class="lazyload" data-src="images/book-images/no-subsampling-large.jpg" alt="JPEG includes support for numerous subsampling types: none, horizontal and horizontal and vertical."><noscript><img src="images/book-images/no-subsampling-large.jpg"></noscript></picture><figcaption>JPEG supports a number of different subsampling types: none, horizontal and horizontal and vertical. This diagram is from <a href="http://frdx.free.fr/JPEG_for_the_horseshoe_crabs.pdf">JPEGs for the horseshoe crabs</a> by Frédéric Kayser.</figcaption></figure></p><p>There are a number of common samples discussed when talking about subsampling. Generally, <code>4:4:4</code>, <code>4:2:2</code> and <code>4:2:0</code>. But what do these represent? Let’s say a subsample takes the format A:B:C. A is the number of pixels in a row and for JPEGs this is usually 4. B represents the amount of color in the first row and C the color in the second.<ul><li><code>4:4:4</code> has no compression, so color and luma are transported completely.</li><li><code>4:2:2</code> has half sampling horizontally and full sampling vertically.</li><li><code>4:2:0</code> samples colors out of half the first row’s pixels and ignores the second row.</li></ul><aside class="note"><b>Note:</b> jpegtran and cjpeg support separate quality configuration of luminance and chroma. This can be done adding the <code>-sample</code> flag (e.g. <code>-sample 2x1</code>). Some good general rules: subsampling (<code>-sample 2x2</code>) is great for photos. no-subsampling (<code>-sample 1x1</code>) is best for screenshots, banners and buttons. There’s finally compromise (<code>2x1</code>) is you’re unsure what to use.</aside></p><p>By reducing pixels in our chroma components, it’s possible to reduce the size of color components significantly, ultimately reducing byte size.<figure><picture><source data-srcset="images/book-images/subsampling-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/subsampling-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/subsampling-large.jpg"><img class="lazyload" data-src="images/book-images/subsampling-large.jpg" alt="Chrome subsampling configurations for a JPEG at quality 80."><noscript><img src="images/book-images/subsampling-large.jpg"></noscript></picture><figcaption>Chrome subsampling configurations for a JPEG at quality 80.</figcaption></figure></p><p>Chroma subsampling is worth considering for most types of image. It does have some notable exceptions: as subsampling relies on limitations in our eyes, it is not great for compressing images where color detail may be as important as luminance (e.g. medical images).</p><p>Images containing typefaces can also suffer as poor subsampling of text can decrease its legibility. Sharper edges are harder to compress with JPEG as it was designed to better handle photographic scenes with softer transitions.<figure><picture><source data-srcset="images/book-images/Screen_Shot_2017-08-25_at_11.06.27_AM-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Screen_Shot_2017-08-25_at_11.06.27_AM-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Screen_Shot_2017-08-25_at_11.06.27_AM-large.jpg"><img class="lazyload small" data-src="images/book-images/Screen_Shot_2017-08-25_at_11.06.27_AM-large.jpg" alt="Be careful when using heavy subsampling with images containing text"><noscript><img src="images/book-images/Screen_Shot_2017-08-25_at_11.06.27_AM-large.jpg"></noscript></picture><figcaption><a href="http://compress-or-die.com/Understanding-JPG/">Understanding JPEG</a> recommends sticking with a subsampling of 4:4:4 (1×1) when working with images containing text.</figcaption></figure></p><p>Trivia: The exact method of Chroma subsampling wasn’t specified in the JPEG specification, so different decoders handle it differently. MozJPEG and libjpeg-turbo use the same scaling method. Older versions of libjpeg use a different method that adds ringing artifacts in colors.<aside class="note"><b>Note:</b> Photoshop sets Chroma subsampling automatically when using the ‘Save for web’ feature. When image quality is set between 51-100, no subsampling is used at all (<code>4:4:4</code>). When quality is below this, a <code>4:2:0</code> subsampling is used instead. This is one reason a far greater file-size reduction can be observed when switching quality from 51 to 50.</aside><aside class="note"><b>Note:</b> In subsampling discussions the term <a href="https://en.wikipedia.org/wiki/YCbCr">YCbCr</a> is often mentioned. This is a model that can represent gamma-corrected <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a> color spaces. Y is gamma-corrected luminance, Cb is the blue color’s chroma component and Cr is the red color’s chroma component. If you look at ExifData, you’ll see YCbCr next to sampling levels.</aside></p><p>For a further read on Chroma Subsampling, see <a href="https://calendar.perfplanet.com/2015/why-arent-your-images-using-chroma-subsampling/">Why aren’t your images using Chroma subsampling?</a>.</p><h3 id="-a-id-how-far-have-we-come-from-the-jpeg-href-how-far-have-we-come-from-the-jpeg-how-far-have-we-come-from-the-jpeg-a-"><a id="how-far-have-we-come-from-the-jpeg" href="#how-far-have-we-come-from-the-jpeg">How far have we come from the JPEG?</a></h3><p><strong>Here’s the current state of image formats on the web:</strong></p><p><em>tl;dr – there’s a lot of fragmentation. We often need to conditionally serve different formats to different browsers to take advantage of anything modern.</em><figure><picture><source data-srcset="images/book-images/format-comparison-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/format-comparison-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/format-comparison-large.jpg"><img class="lazyload" data-src="images/book-images/format-comparison-large.jpg" alt="modern image formats compared based on quality."><noscript><img src="images/book-images/format-comparison-large.jpg"></noscript></picture><figcaption>Different modern image formats (and optimizers) used to demonstrate what is possible at a target file-size of 26KB. We can compare quality using <a href="https://en.wikipedia.org/wiki/Structural_similarity">SSIM</a> (structural similarity) or <a href="https://github.com/google/butteraugli">Butteraugli</a>, which we’ll cover in more detail later.</figcaption></figure><ul><li><strong><a href="https://en.wikipedia.org/wiki/JPEG_2000">JPEG 2000</a> (2000)</strong> – an improvement to JPEG switching from a discrete cosine based transform to a wavelet-based method. <strong>Browser support: Safari desktop + iOS</strong></li><li><strong><a href="https://en.wikipedia.org/wiki/JPEG_XR">JPEG XR</a> (2009)</strong> – alternative to JPEG and JPEG 2000 supporting <a href="http://wikivisually.com/wiki/High_dynamic_range_imaging">HDR</a> and wide <a href="http://wikivisually.com/wiki/Gamut">gamut</a> color spaces. Produces smaller files than JPEG at slightly slower encode/decode speeds. <strong>Browser support: Edge, IE.</strong></li><li><strong><a href="https://en.wikipedia.org/wiki/WebP">WebP</a> (2010)</strong> – block-prediction based format by Google with support for lossy and lossless compression. Offers byte savings associated with JPEG and transparency support byte-heavy PNGs are often used for. Lacks chroma subsampling configuration and progressive loading. Decode times are also slower than JPEG decoding. <strong>Browser support: Chrome, Opera. Experimented with by Safari and Firefox.</strong></li><li><strong><a href="https://en.wikipedia.org/wiki/Free_Lossless_Image_Format">FLIF</a> (2015)</strong> – lossless image format claiming to outperform PNG, lossless WebP, lossless BPG and lossless JPEG 2000 based on compression ratio. <strong>Browser support: none. <em>Note that there is a <a href="https://github.com/UprootLabs/poly-flif">JS in-browser decoder</a>.</em></strong></li><li><strong>HEIF and BPG.</strong> From a compression perspective, they’re the same but have a different wrapper:</li><li><strong><a href="https://en.wikipedia.org/wiki/Better_Portable_Graphics">BPG</a> (2015)</strong> – intended to be more compression-efficient replacement for JPEG, based on HEVC (<a href="http://wikivisually.com/wiki/High_Efficiency_Video_Coding">High Efficiency Video Coding</a>). Appears to offer better file size compared to MozJPEG and WebP. Unlikely to get broad traction due to licensing issues. <strong>Browser support: none. <em>Note that there is a <a href="https://bellard.org/bpg/">JS in-browser decoder</a>.</em></strong></li><li><strong><a href="https://en.wikipedia.org/wiki/High_Efficiency_Image_File_Format">HEIF</a> (2015)</strong> – format for images and image sequences for storing HEVC-encoded images with constrained inter-prediction applied. Apple announced at <a href="https://www.cnet.com/news/apple-ios-boosts-heif-photos-over-jpeg-wwdc/">WWDC</a> they would explore switching to HEIF over JPEG for iOS, citing up to 2× savings on file-size. <strong>Browser support: None at the time of writing. Eventually, Safari desktop and iOS 11</strong></li></ul></p><p>If you’re more visual, you might appreciate <a href="https://people.xiph.org/~xiphmont/demo/daala/update1-tool2b.shtml">one</a> of <a href="http://xooyoozoo.github.io/yolo-octo-bugfixes/#cologne-cathedral&amp;jpg=s&amp;webp=s">these</a> visual comparison tools for some of the above.</p><p>So, <strong>browser support is fragmented</strong> and if you wish to take advantage of any of the above you’ll likely need to conditionally serve fallbacks for each of your target browsers. At Google, we’ve seen some promise with WebP so we’ll dive into it in more depth shortly.</p><p>You can also serve image formats (e.g. WebP, JPEG 2000) with a .jpg extension (or any other) as the browser can render an image it can decide the media type. This allows for server-side <a href="https://www.igvita.com/2012/12/18/deploying-new-image-formats-on-the-web/">content-type negotiation</a> to decide which image to send without needing to change the HTML at all. Services like Instart Logic use this approach when delivering images to their customers.</p><p>Next, let’s talk about an option for when you can’t conditionally serve different image formats: <strong>optimising JPEG encoders</strong>.</p><h3 id="-a-id-optimizing-jpeg-encoders-href-optimizing-jpeg-encoders-optimizing-jpeg-encoders-a-"><a id="optimizing-jpeg-encoders" href="#optimizing-jpeg-encoders">Optimizing JPEG Encoders</a></h3><p>Modern JPEG encoders attempt to produce smaller, higher fidelity JPEG files while maintaining compatibility with existing browsers and image processing apps. They avoid the need to introduce new image formats or changes in the ecosystem in order for compression gains to be possible. Two such encoders are MozJPEG and Guetzli.</p><p><strong><em>tl;dr Which optimising JPEG Encoder should you use?</em></strong><ul><li>General web assets: MozJPEG</li><li>Quality is your key concern and you don’t mind long encode times: use Guetzli</li><li>If you need configurability:<ul><li><a href="https://github.com/danielgtaylor/jpeg-archive">JPEGRecompress</a> (which uses MozJPEG under the hood)</li><li><a href="http://www.jpegmini.com/">JPEGMini</a>. It’s similar to Guetzli – chooses best quality automatically. It’s not as technically sophisticated as Guetzli, but it’s faster, and aims at quality range more suitable for the web.</li><li><a href="https://imageoptim.com/api">ImageOptim API</a> (with free online interface <a href="https://imageoptim.com/online">here</a>) – it’s unique in its handling of color. You can choose color quality separately from overall quality. It automatically chooses chroma subsampling level to preserve high-res colors in screenshots, but avoid waste bytes on smooth colors in natural photos.</li></ul></li></ul></p><h3 id="-a-id-what-is-mozjpeg-href-what-is-mozjpeg-what-is-mozjpeg-a-"><a id="what-is-mozjpeg" href="#what-is-mozjpeg">What is MozJPEG?</a></h3><p>Mozilla offers a modernized JPEG encoder in the form of <a href="https://github.com/mozilla/mozjpeg">MozJPEG</a>. It <a href="https://research.mozilla.org/2014/03/05/introducing-the-mozjpeg-project/">claims</a> to shave up to 10% off JPEG files. Files compressed with MozJPEG work cross-browser and some of its features include progressive scan optimization, <a href="https://en.wikipedia.org/wiki/Trellis_quantization">trellis quantization</a> (discarding details that compress the least) and a few decent <a href="https://calendar.perfplanet.com/2014/mozjpeg-3-0/">quantization table presets</a> that help create smoother High-DPI images (although this is possible with ImageMagick if you’re willing to wade through XML configurations).</p><p>MozJPEG is supported in both <a href="https://github.com/ImageOptim/ImageOptim/issues/45">ImageOptim</a> and there’s a relatively reliable configurable <a href="https://github.com/imagemin/imagemin-mozjpeg">imagemin plugin</a> for it. Here’s a sample implementation with Gulp:<pre><code class="lang-js">const gulp = require(&#39;gulp&#39;);
const imagemin = require(&#39;gulp-imagemin&#39;);
const imageminMozjpeg = require(&#39;imagemin-mozjpeg&#39;);

gulp.task(&#39;mozjpeg&#39;, () =&gt;
    gulp.src(&#39;src/*.jpg&#39;)
    .pipe(imagemin([imageminMozjpeg({
        quality: 85
    })]))
    .pipe(gulp.dest(&#39;dist&#39;))
);
</code></pre><figure><picture><source data-srcset="images/book-images/Modern-Image10-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image10-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image10-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image10-large.jpg" alt="mozjpeg being run from the command-line"><noscript><img src="images/book-images/Modern-Image10-large.jpg"></noscript></picture></figure><figure><picture><source data-srcset="images/book-images/Modern-Image11-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image11-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image11-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image11-large.jpg" alt="mozjpeg compression at different qualities. At q=90, 841KB. At q=85, 562KB. At q=75, 324KB. Similarly, Butteraugli and SSIM scores get slightly worse as we lower quality."><noscript><img src="images/book-images/Modern-Image11-large.jpg"></noscript></picture><figcaption>MozJPEG: A comparison of file-sizes and visual similarity scores at different qualities.</figcaption></figure></p><p>I used <a href="https://github.com/imagemin/imagemin-jpeg-recompress">jpeg-compress</a> from the <a href="https://github.com/danielgtaylor/jpeg-archive">jpeg-archive</a> project to calculate the SSIM (The Structural Similarity) scores for a source image. SSIM is a method for measuring the similarity between two images, where the SSIM score is a quality measure of one image given the other is considered ‘perfect’.</p><p>In my experience, MozJPEG is a good option for compressing images for the web at a high visual quality while delivering reductions on file size. For small to medium sized images, I found MozJPEG (at quality=80-85) led to 30-40% savings on file size while maintaining acceptable SSIM, offering a 5-6% improvement on jpeg-turbo. It does come with a <a href="http://www.libjpeg-turbo.org/About/Mozjpeg">slower encoding cost</a> than baseline JPEG, but you may not find this a show stopper.<aside class="note"><b>Note:</b> if you need a tool supporting MozJPEG with additional configuration support and some complimentary utilities for image comparison, check out <a href="https://github.com/danielgtaylor/jpeg-archive">jpeg-recompress</a>. Jeremy Wagner, author of Web Performance in Action has had some success using it with <a href="https://twitter.com/malchata/status/884836650563579904">this</a> configuration.</aside></p><h3 id="-a-id-what-is-guetzli-href-what-is-guetzli-what-is-guetzli-a-"><a id="what-is-guetzli" href="#what-is-guetzli">What is Guetzli?</a></h3><p><a href="https://github.com/google/guetzli">Guetzli</a> is a promising, if slow, perceptual JPEG encoder from Google that tries to find the smallest JPEG that is perceptually indistinguishable from the original to the human eye. It performs a sequence of experiments that produces a proposal for the final JPEG, accounting for the psychovisual error of each proposal. Out of these, it selects the highest-scoring proposal as the final output.</p><p>To measure the differences between images, Guetzli uses <a href="https://github.com/google/butteraugli">Butteraugli</a>, a model for measuring image difference based on human perception (discussed below). Guetzli can take into account a few properties of vision that other JPEG encoders do not. For example, there is a relationship between the amount of green light seen and sensitivity to blue, so changes in blue in the vicinity of green can be encoded a little less precisely.<aside class="note"><b>Note:</b> Image file-size is <strong>much</strong> more dependent on the choice of <strong>quality</strong> than the choice of <strong>codec</strong>. There are far far larger file-size differences between the lowest and highest quality JPEGs compared to the file-size savings made possible by switching codecs. Using the lowest acceptable quality is very important. Avoid setting your quality too high without paying attention to it.</aside></p><p>Guetzli <a href="https://research.googleblog.com/2017/03/announcing-guetzli-new-open-source-jpeg.html">claims</a> to achieve a 20-30% reduction in data-size for images for a given Butteraugli score compared to other compressors. A large caveat to using Guetzli is that it is extremely, extremely slow and is currently only suitable for static content. From the README, we can note Guetzli requires a large amount of memory – it can take 1 minute + 200MB RAM per megapixel. There’s a good thread on real-world experience with Guetzli in <a href="https://github.com/google/guetzli/issues/50">this GitHub thread</a>. It can be ideal for when you’re optimizing images as part of a build process for a static site but less so when performed on demand.<aside class="note"><b>Note:</b> Guetzli may be more suitable when you’re optimizing images as part of a build process for a static site, or situations where image optimization is not performed on demand.</aside></p><p>Tools like ImageOptim support Guetzli optimization (in <a href="https://imageoptim.com/">the latest versions</a>).<pre><code class="lang-js">const gulp = require(&#39;gulp&#39;);
const imagemin = require(&#39;gulp-imagemin&#39;);
const imageminGuetzli = require(&#39;imagemin-guetzli&#39;);

gulp.task(&#39;guetzli&#39;, () =&gt;
    gulp.src(&#39;src/*.jpg&#39;)
    .pipe(imagemin([
        imageminGuetzli({
            quality: 85
        })
    ]))
    .pipe(gulp.dest(&#39;dist&#39;))

);
</code></pre><figure><picture><source data-srcset="images/book-images/Modern-Image12-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image12-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image12-large.jpg"><img class="small lazyload" data-src="images/book-images/Modern-Image12-large.jpg" alt="guetzli being run from gulp for optimisation"><noscript><img src="images/book-images/Modern-Image12-large.jpg"></noscript></picture></figure></p><p>It took almost seven minutes (and high CPU usage) to encode 3 x 3MP images using Guetzli with varied savings. For archiving higher-resolution photos, I could see this offering some value.<figure><picture><source data-srcset="images/book-images/Modern-Image13-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image13-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image13-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image13-large.jpg" alt="comparison of guetzli at different qualities. q=100, 945KB. q=90, 687KB. q=85, 542KB."><noscript><img src="images/book-images/Modern-Image13-large.jpg"></noscript></picture><figcaption>Guetzli: A comparison of file sizes and visual similarity scores at different qualities.</figcaption></figure><aside class="note"><b>Note:</b> It’s recommended to run Guetzli on high quality images (e.g. uncompressed input images, PNG sources or JPEGs of 100% quality or close). While it will work on other images (e.g. JPEGs of quality 84 or lower), results can be poorer.</aside></p><p>While compressing an image with Guetzli is very (very) time-consuming and will make your fans spin, for larger images, it is worth it. I have seen a number of examples where it saved anywhere up to 40% on file size while maintaining visual fidelity. This made it perfect for archiving photos. On small to medium sized images, I have still seen some savings (in the 10-15KB range) but they were not quite as well pronounced. Guetzli can introduce more liquify-esque distortion on smaller images while compressing.</p><p>You may also be interested in Eric Portis research <a href="https://cloudinary.com/blog/a_closer_look_at_guetzli">comparing</a> Guetzli to Cloudinary’s auto-compression for a different data point on effectiveness.</p><h3 id="-a-id-mozjpeg-vs-guetzli-href-mozjpeg-vs-guetzli-how-does-mozjpeg-compare-to-guetzli-a-"><a id="mozjpeg-vs-guetzli" href="#mozjpeg-vs-guetzli">How does MozJPEG compare to Guetzli?</a></h3><p>Comparing different JPEG encoders is complex – one needs to compare both the quality and fidelity of the compressed image as well as the final size. As image compression expert Kornel Lesi&#x144;ski notes, benchmarking one but not both of these aspects could lead to <a href="https://kornel.ski/faircomparison">invalid</a> conclusions.</p><p>How does Guetzli compare to MozJPEG? – Kornel’s take:<ul><li>Guetzli is tuned for higher-quality images (butteraugli is said to be best for <code>q=90</code>+, MozJPEG’s sweet spot is around <code>q=75</code>)</li><li>Guetzli is much slower to compress (both produce standard JPEGs, so decoding is fast as usual)</li><li>MozJPEG doesn’t automagically pick quality setting, but you can find optimal quality using an external tool, e.g. <a href="https://github.com/danielgtaylor/jpeg-archive">jpeg-archive</a></li></ul></p><p>A number of methods exist for determining if compressed images are visually similar or perceivably similar to their sources. Image quality studies often use methods like <a href="https://en.wikipedia.org/wiki/Structural_similarity">SSIM</a> (structural similarity). Guetzli however optimizes for Butteraugli.</p><h3 id="-a-id-butteraugli-href-butteraugli-butteraugli-a-"><a id="butteraugli" href="#butteraugli">Butteraugli</a></h3><p><a href="https://github.com/google/butteraugli">Butteraugli</a> is a project by Google that estimates the point when a person may notice visual image degradation (the psychovisual similarity) of two images. It gives a score for the images that is reliable in the domain of barely noticeable differences. Butteraugli not only gives a scalar score, but also computes a spatial map of the level of differences. While SSIM looks at the aggregate of errors from an image, Butteraugli looks at the worst part.<figure><picture><source data-srcset="images/book-images/Modern-Image14-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image14-large.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image14-medium.jpg"><img class="lazyload small" data-src="images/book-images/Modern-Image14-medium.jpg" alt="butteraugli validating an image of a parrot"><noscript><img src="images/book-images/Modern-Image14-medium.jpg"></noscript></picture><figcaption>Above is an example that used Butteraugli to find the minimal JPEG quality threshold before visual degradation was bad enough for a user to notice something wasn’t clear. It resulted in a 65% reduction in total file size.</figcaption></figure></p><p>In practice, you would define a target goal for visual quality and then run through a number of different image optimisation strategies, looking at your Butteraugli scores, before choosing something that fits the best balance of file- size and level.<figure><picture><source data-srcset="images/book-images/Modern-Image15-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image15-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image15-large.jpg"><img class="lazyload small" data-src="images/book-images/Modern-Image15-large.jpg" alt="butteraugli being run from the command line"><noscript><img src="images/book-images/Modern-Image15-large.jpg"></noscript></picture><figcaption>All in all, it took me about 30m to setup Butteraugli locally after installing Bazel and getting a build of the C++ sources to correctly compile on my Mac. Using it is then relatively straight-forward: specify the two images to compare (a source and compressed version) and it will give you a score to work from.</figcaption></figure></p><p><strong>How does Butteraugli differ to other ways of comparing visual similarity?</strong></p><p><a href="https://github.com/google/guetzli/issues/10#issuecomment-276295265">This comment</a> from a Guetzli project member suggests Guetzli scores best on Butteraugli, worst on SSIM and MozJPEG scores about as well on both. This is in line with the research I’ve put into my own image optimisation strategy. I run Butteraugli and a Node module like <a href="https://www.npmjs.com/package/img-ssim">img-ssim</a> over images comparing the source to their SSIM scores before/after Guetzli and MozJPEG.</p><p><strong>Combining encoders?</strong></p><p>For larger images, I found combining Guetzli with <strong>lossless compression </strong>in MozJPEG (jpegtran, not cjpeg to avoid throwing away the work done by Guetzli) can lead to a further 10-15% decrease in filesize (55% overall) with only very minor decreases in SSIM. This is something I would caution requires experimentation and analysis but has also been tried by others in the field like <a href="https://ariya.io/2017/03/squeezing-jpeg-images-with-guetzli">Ariya Hidayat</a> with promising results.</p><p>MozJPEG is a beginner-friendly encoder for web assets that is relatively fast and produces good-quality images. As Guetzli is resource-intensive and works best on larger, higher-quality images, it’s an option I would reserve for intermediate to advanced users.</p><h2 id="-a-id-what-is-webp-href-what-is-webp-what-is-webp-a-"><a id="what-is-webp" href="#what-is-webp">What is WebP?</a></h2><p><a href="https://developers.google.com/speed/webp/">WebP</a> is a recent image format from Google aiming to offer lower file-sizes for lossless and lossy compression at an acceptable visual quality. It includes support for alpha-channel transparency and animation.</p><p>In the last year, WebP gained a few percent over compression-wise in lossy and lossless modes and speed-wise the algorithm got twice as fast with a 10% improvement in decompression. WebP is not a tool for all purposes, but it has some standing and a growing user base in the image compression community. Let’s examine why.<figure><picture><source data-srcset="images/book-images/Modern-Image16-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image16-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image16-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image16-large.jpg" alt="comparison of webp at different quality settings. q=90, 646KB. q=80= 290KB. q=75, 219KB. q=70, 199KB"><noscript><img src="images/book-images/Modern-Image16-large.jpg"></noscript></picture><figcaption>WebP: A comparison of file sizes and visual similarity scores at different qualities.</figcaption></figure></p><h3 id="-a-id-how-does-webp-perform-href-how-does-webp-perform-how-does-webp-perform-a-"><a id="how-does-webp-perform" href="#how-does-webp-perform">How does WebP perform?</a></h3><p><strong>Lossy Compression</strong></p><p>WebP lossy files, using a VP8 or VP9 video key frame encoding variant, are on average cited by the WebP team as being <a href="https://developers.google.com/speed/webp/docs/webp_study">25-34%</a> smaller than JPEG files.</p><p>In the low-quality range (0-50), WebP has a large advantage over JPEG because it can blur away ugly blockiness artifacts. A medium quality setting (-m 4 -q 75) is the default balancing speed/file-size. In the higher-range (80-99), the advantages of WebP shrink. WebP is recommended where speed matters more than quality.</p><p><strong>Lossless Compression</strong></p><p><a href="https://developers.google.com/speed/webp/docs/webp_lossless_alpha_study">WebP lossless files are 26% smaller than PNG files</a>. The lossless load-time decrease compared to PNG is 3%. That said, you generally don’t want to deliver your users lossless on the web. There’s a difference between lossless and sharp edges (e.g. non-JPEG). Lossless WebP may be more suitable for archival content.</p><p><strong>Transparency</strong></p><p>WebP has a lossless 8-bit transparency channel with only 22% more bytes than PNG. It also supports lossy RGB transparency, which is a feature unique to WebP.</p><p><strong>Metadata</strong></p><p>The WebP file format supports EXIF photo metadata and XMP digital document metadata. It also contains an ICC Color Profile.</p><p>WebP offers better compression at the cost of being more CPU intensive. Back in 2013, the compression speed of WebP was ~10× slower than JPEG but is now negligible (some images may be 2× slower). For static images that are processed as part of your build, this shouldn’t be a large issue. Dynamically generated images will likely cause a perceivable CPU overhead and will be something you will need to evaluate.<aside class="note"><b>Note:</b> WebP lossy quality settings are not directly comparable to JPEG. A JPEG at ‘70% quality’ will be quite different to a WebP image at ‘70% quality’ because WebP achieves smaller file sizes by discarding more data.</aside></p><h3 id="-a-id-whos-using-webp-in-production-href-whos-using-webp-in-production-who-s-using-webp-in-production-a-"><a id="whos-using-webp-in-production" href="#whos-using-webp-in-production">Who’s using WebP in production?</a></h3><p>Many large companies are using WebP in production to reduce costs and decrease web page load times.</p><p>Google reported 30-35% savings using WebP over other lossy compression schemes, serving 43 billion image requests a day, 26% of that being lossless compression. That’s a lot of requests and significant savings. Savings would undoubtedly be larger if <a href="http://caniuse.com/#search=webp">browser support</a> were better and more widespread. Google also uses it in production sites like Google Play and YouTube.</p><p>Netflix, Amazon, Quora, Yahoo, Walmart, Ebay, The Guardian, Fortune, and USA Today, all compress and serve images with WebP for browsers which support it. VoxMedia <a href="https://product.voxmedia.com/2015/8/13/9143805/performance-update-2-electric-boogaloo">shaved 1-3s off load times</a> for The Verge by switching over to WebP for their Chrome users. <a href="https://iso.500px.com/500px-color-profiles-file-formats-and-you/">500px</a> saw an average 25% reduction in image file size with similar or better image quality when switching to serving it to their Chrome users.</p><p>There are quite a few more companies on board than this sample list indicates.<figure><picture><source data-srcset="images/book-images/webp-conversion-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/webp-conversion-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/webp-conversion-large.jpg"><img class="small lazyload" data-src="images/book-images/webp-conversion-large.jpg" alt="WebP stats at Google: over 43B image requests a day"><noscript><img src="images/book-images/webp-conversion-large.jpg"></noscript></picture><figcaption>WebP usage at Google: 43 billion WebP image requests a day are served across YouTube, Google Play, Chrome Data Saver and G+.</figcaption></figure></p><h3 id="-a-id-how-does-webp-encoding-work-href-how-does-webp-encoding-work-how-does-webp-encoding-work-a-"><a id="how-does-webp-encoding-work" href="#how-does-webp-encoding-work">How does WebP encoding work?</a></h3><p>WebP’s lossy encoding is designed to compete with JPEG for still images. There are three key phases to WebP’s lossy encoding:</p><p><strong>Macro-blocking</strong> – splitting an image into 16×16 (macro) blocks of luma pixels and two 8×8 blocks of chroma pixels. This may sound familiar to the idea of JPEGs doing color space conversion, chroma channel downsampling and image subdivision.<figure><picture><source data-srcset="images/book-images/Modern-Image18-small.png" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image18-large.png" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image18-medium.png"><img class="small lazyload" data-src="images/book-images/Modern-Image18-medium.png" alt="Macro-blocking example of a Google Doodle where we break a range of pixels down into luma and chroma blocks."><noscript><img src="images/book-images/Modern-Image18-medium.png"></noscript></picture></figure></p><p><strong>Prediction</strong> – every 4×4 subblock of a macroblock has a prediction model applied that effectively does filtering. This defines two sets of pixels around a block – A (the row directly above it) and L (the column to the left of it). Using these two the encoder fills a test block with 4×4 pixels and determines which creates values closest to the original block. Colt McAnlis talks about this in more depth in <a href="https://medium.com/@duhroach/how-webp-works-lossly-mode-33bd2b1d0670">How WebP lossy mode works</a>.<figure><picture><source data-srcset="images/book-images/Modern-Image19-medium.png" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image19-large.png" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image19-small.png"><img class="lazyload very-small" data-src="images/book-images/Modern-Image19-small.png" alt="Google Doodle example of a segment displaying the row, target block and column L when considering a prediction model."><noscript><img src="images/book-images/Modern-Image19-small.png"></noscript></picture></figure></p><p>A Discrete Cosine Transform (DCT) is applied with a few steps similar to JPEG encoding. A key difference is use of an <a href="https://www.youtube.com/watch?v=FdMoL3PzmSA&amp;index=7&amp;list=PLOU2XLYxmsIJGErt5rrCqaSGTMyyqNt2H">Arithmetic Compressor</a> vs JPEG’s Huffman.</p><p>If you want to dive deeper, Google Developer’s article <a href="https://developers.google.com/speed/webp/docs/compression">WebP Compression Techniques</a> goes into this topic in depth.</p><h3 id="-a-id-webp-browser-support-href-webp-browser-support-webp-browser-support-a-"><a id="webp-browser-support" href="#webp-browser-support">WebP browser support</a></h3><p>Not all browsers support WebP, however <a href="http://caniuse.com/webp">according to CanIUse.com</a>, global user support is at about 74%. Chrome and Opera natively support it. Safari, Edge, and Firefox have experimented with it but not landed it yet in official releases. This often leaves the task of getting the WebP image to the user up to the web developer. More on this later.</p><p>Here are the major browsers and support information for each:<ul><li>Chrome: Chrome began full support at version 23.</li><li>Chrome for Android: Since Chrome 50</li><li>Android: Since Android 4.2</li><li>Opera: Since 12.1</li><li>Opera Mini: All versions</li><li>Firefox: Some beta support</li><li>Edge: Some beta support</li><li>Internet Explorer: No support</li><li>Safari: Some beta support</li></ul></p><p>WebP is not without its downsides. It lacks full-resolution color space options and does not support progressive decoding. That said, tooling for WebP is decent and browser-support, while limited to Chrome and Opera at the time of writing, may well cover enough of your users for it to be worth considering with a fallback.</p><h3 id="-a-id-how-do-i-convert-to-webp-href-how-do-i-convert-to-webp-how-do-i-convert-my-images-to-webp-a-"><a id="how-do-i-convert-to-webp" href="#how-do-i-convert-to-webp">How do I convert my images to WebP?</a></h3><p>Several commercial and open source image editing and processing packages support WebP. One particularly useful application is XnConvert: a free, cross-platform, batch image processing converter.<aside class="note"><b>Note:</b> It’s important to avoid converting low or average quality JPEGs to WebP. This is a common mistake and can generate WebP images with JPEG compression artifacts. This can lead to WebP being less efficient as it has to save the image <em>and</em> the distortions added by JPEG, leading to you losing on quality twice. Feed conversion apps the best quality source file available, preferably the original.</aside></p><p><strong><a href="http://www.xnview.com/en/xnconvert/">XnConvert</a></strong></p><p>XnConvert enables batch image processing, compatible with over 500 image formats. You can combine over 80 separate actions to transform or edit your images in multiple ways.<figure><picture><source data-srcset="images/book-images/Modern-Image20-small.png" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image20-medium.png" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image20-large.png"><img class="small lazyload" data-src="images/book-images/Modern-Image20-large.png" alt="XNConvert app on Mac where a number of images have been converted to WebP"><noscript><img src="images/book-images/Modern-Image20-large.png"></noscript></picture><figcaption>XnConvert supports batch image optimisation, allowing straight-forward conversion from source files to WebP and other formats. In addition to compression, XnConvert can also help with metadata stripping, cropping, color depth customisation and other transforms.</figcaption></figure></p><p>Some of the options listed on the xnview website include:<ul><li>Metadata: Editing</li><li>Transforms: Rotate, Crop, Resize</li><li>Adjustments: Brightness, Contrast, Saturation</li><li>Filters: Blur, Emboss, Sharpen</li><li>Effects: Masking, Watermark, Vignetting</li></ul></p><p>The results of your operations can be exported to about 70 different file formats, including WebP. XnConvert is free for Linux, Mac, and Windows. XnConvert is highly recommended, especially for small businesses.</p><p><strong>Node modules</strong></p><p><a href="https://github.com/imagemin/imagemin">Imagemin</a> is a popular image minification module that also has an add-on for converting images to WebP (<a href="https://github.com/imagemin/imagemin-webp">imagemin-webp</a>). This supports both lossy and lossless modes.</p><p>To install imagemin and imagemin-webp run:<pre><code>&gt; npm install --save imagemin imagemin-webp
</code></pre></p><p>We can then require() in both modules and run them over any images (e.g. JPEGs) in a project directory. Below we’re using lossy encoding with a WebP encoder quality of 60:<pre><code class="lang-js">const imagemin = require(&#39;imagemin&#39;);
const imageminWebp = require(&#39;imagemin-webp&#39;);

imagemin([&#39;images/*.{jpg}&#39;], &#39;images&#39;, {
    use: [
        imageminWebp({quality: 60})
    ]
}).then(() =&gt; {
    console.log(‘Images optimized’);
});
</code></pre></p><p>Similar to JPEGs, it’s possible to notice compression artefacts in our output. Evaluate what quality setting makes sense for your own images. Imagemin-webp can also be used to encode lossless quality WebP images (supporting 24-bit color and full transparency) by passing <code>lossless: true</code> to options:<pre><code class="lang-js">const imagemin = require(&#39;imagemin&#39;);
const imageminWebp = require(&#39;imagemin-webp&#39;);

imagemin([&#39;images/*.{jpg,png}&#39;], &#39;build/images&#39;, {
    use: [
        imageminWebp({lossless: true})
    ]
}).then(() =&gt; {
    console.log(‘Images optimized’);
});
</code></pre></p><p>A <a href="https://github.com/sindresorhus/gulp-webp">WebP plugin for Gulp</a> by Sindre Sorhus built on imagemin-webp and a <a href="https://www.npmjs.com/package/webp-loader">WebP loader for WebPack</a> are also available. The Gulp plugin accepts any options the imagemin add-on does:<pre><code class="lang-js">const gulp = require(&#39;gulp&#39;);
const webp = require(&#39;gulp-webp&#39;);

gulp.task(&#39;webp&#39;, () =&gt;
    gulp.src(&#39;src/*.jpg&#39;)
    .pipe(webp({
        quality: 80,
        preset: &#39;photo&#39;,
        method: 6
    }))
    .pipe(gulp.dest(&#39;dist&#39;))
);
</code></pre></p><p>Or lossless:<pre><code class="lang-js">const gulp = require(&#39;gulp&#39;);
const webp = require(&#39;gulp-webp&#39;);

gulp.task(&#39;webp-lossless&#39;, () =&gt;
    gulp.src(&#39;src/*.jpg&#39;)
    .pipe(webp({
        lossless: true
    }))
    .pipe(gulp.dest(&#39;dist&#39;))
);
</code></pre></p><p><strong>Batch image optimization using Bash</strong></p><p>XNConvert supports batch image compression, but if you would prefer to avoid using an app or a build system, bash and image optimization binaries keep things fairly simple.</p><p>You can bulk convert your images to WebP using <a href="https://developers.google.com/speed/webp/docs/cwebp">cwebp</a>:<pre><code>find ./ -type f -name &#39;*.jpg&#39; -exec cwebp -q 70 {} -o {}.webp \;
</code></pre></p><p>Or bulk optimize your image sources with MozJPEG using <a href="https://github.com/danielgtaylor/jpeg-archive">jpeg-recompress</a>:<pre><code>find ./ -type f -name &#39;*.jpg&#39; -exec jpeg-recompress {} {} \;
</code></pre></p><p>and trim those SVGs down using <a href="https://github.com/svg/svgo">svgo</a> (which we’ll cover later on):<pre><code>find ./ -type f -name &#39;*.svg&#39; -exec svgo {} \;
</code></pre></p><p>Jeremy Wagner has a more comprehensive post on <a href="https://jeremywagner.me/blog/bulk-image-optimization-in-bash">image optimization using Bash</a> and another on doing this work in <a href="https://jeremywagner.me/blog/faster-bulk-image-optimization-in-bash">parallel</a> worth reading.</p><p><strong>Other WebP image processing and editing apps include:</strong><ul><li>Leptonica — An entire website of open source image processing and analysis Apps.</li></ul><ul><li>Sketch supports outputting directly to WebP<ul><li>GIMP — Free, open source Photoshop alternative. Image editor.</li><li>ImageMagick — Create, compose, convert, or edit bitmap images. Free. Command-Line app.</li><li>Pixelmator — Commercial image editor for Mac.</li><li>Photoshop WebP Plugin — Free. Image import and export. From Google.</li></ul></li></ul></p><p><strong>Android:</strong> You can convert existing BMP, JPG, PNG or static GIF images to WebP format using Android Studio. For more information, see <a href="https://developer.android.com/studio/write/convert-webp.html">Create WebP Images Using Android Studio</a>.</p><h3 id="-a-id-how-do-i-view-webp-on-my-os-href-how-do-i-view-webp-on-my-os-how-do-i-view-webp-images-on-my-os-a-"><a id="how-do-i-view-webp-on-my-os" href="#how-do-i-view-webp-on-my-os">How do I view WebP images on my OS?</a></h3><p>While you can drag and drop WebP images to Blink-based browsers (Chrome, Opera, Brave) to preview them, you can also preview them directly from your OS using an add-on for either Mac or Windows.</p><p><a href="https://www.cnet.com/news/facebook-tries-googles-webp-image-format-users-squawk/">Facebook experimented with WebP</a> a few years ago and found that users who tried to right-click on photos and save them to disk noticed they wouldn’t be displayed outside their browser due to them being in WebP. There were three key problems here:<ul><li>&quot;Save as&quot; but unable to view WebP files locally. This was fixed by Chrome registering itself as a &quot;.webp&quot; handler.</li><li>&quot;Save as&quot; then attaching the image to an email and sharing with someone without Chrome. Facebook solved this by introducing a prominent &quot;download&quot; button in their UI and returning a JPEG when users requested the download.</li><li>Right click &gt; copy URL -&gt; share URL on the web. This was solved by <a href="https://www.igvita.com/2012/12/18/deploying-new-image-formats-on-the-web/">content-type negotiation</a>.</li></ul></p><p>These issues might matter less to your users, but is an interesting note on social shareability in passing. Thankfully today, utilities exist for viewing and working with WebP on different operating systems.</p><p>On Mac, try the <a href="https://github.com/Nyx0uf/qlImageSize">Quick Look plugin for WebP</a> (qlImageSize). It works pretty well:<figure><picture><source data-srcset="images/book-images/Modern-Image22-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image22-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image22-large.jpg"><img class="small lazyload" data-src="images/book-images/Modern-Image22-large.jpg" alt="Desktop on a mac showing a WebP file previewed using the Quick Look plugin for WebP files"><noscript><img src="images/book-images/Modern-Image22-large.jpg"></noscript></picture></figure></p><p>On Windows, you can also download the <a href="https://storage.googleapis.com/downloads.webmproject.org/releases/webp/WebpCodecSetup.exe">WebP codec package</a> allowing WebP images to be previewed in the File Explorer and Windows Photo Viewer.</p><h3 id="-a-id-how-do-i-serve-webp-href-how-do-i-serve-webp-how-do-i-serve-webp-a-"><a id="how-do-i-serve-webp" href="#how-do-i-serve-webp">How do I serve WebP?</a></h3><p>Browsers without WebP support can end up not displaying an image at all, which isn’t ideal. To avoid this there are a few strategies we can use for conditionally serving WebP based on browser support.<figure><picture><source data-srcset="images/book-images/play-format-webp-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/play-format-webp-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/play-format-webp-large.jpg"><img class="lazyload" data-src="images/book-images/play-format-webp-large.jpg" alt="The Chrome DevTools Network panel displaying the waterfall for the Play Store in Chrome, where WebP is served."><noscript><img src="images/book-images/play-format-webp-large.jpg"></noscript></picture><figcaption>The Chrome DevTools Network panel highlighting WebP files being conditionally served to Blink-based browsers under the ‘Type’ column.</figcaption></figure><figure><picture><source data-srcset="images/book-images/play-format-type-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/play-format-type-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/play-format-type-large.jpg"><img class="lazyload small" data-src="images/book-images/play-format-type-large.jpg" alt="While the Play store delivers WebP to Blink, it falls back to JPEGs for browsers like Firefox."><noscript><img src="images/book-images/play-format-type-large.jpg"></noscript></picture><figcaption>While the Play store delivers WebP to Blink, it falls back to JPEGs for browsers like Firefox.</figcaption></figure></p><p>Here are some of the options for getting WebP images from your server to your user:</p><p><strong>Using .htaccess to Serve WebP Copies</strong></p><p>Here’s how to use a .htaccess file to serve WebP files to supported browsers when a matching .webp version of a JPEG/PNG file exists on the server.</p><p>Vincent Orback recommended this approach:</p><p>Browsers can <a href="http://vincentorback.se/blog/using-webp-images-with-htaccess/">signal WebP support explicitly</a> via an <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept">Accept header</a>. If you control your backend, you can return a WebP version of an image if it exists on disk rather than formats like JPEG or PNG. This isn’t always possible however (e.g. for static hosts like GitHub pages or S3) so be sure to check before considering this option.</p><p>Here is a sample .htaccess file for the Apache web server:<pre><code>&lt;IfModule mod_rewrite.c&gt;

  RewriteEngine On

  # Check if browser support WebP images
  RewriteCond %{HTTP_ACCEPT} image/webp

  # Check if WebP replacement image exists
  RewriteCond %{DOCUMENT_ROOT}/$1.webp -f

  # Serve WebP image instead
  RewriteRule (.+)\.(jpe?g|png)$ $1.webp [T=image/webp,E=accept:1]

&lt;/IfModule&gt;

&lt;IfModule mod_headers.c&gt;

    Header append Vary Accept env=REDIRECT_accept

&lt;/IfModule&gt;

AddType  image/webp .webp
</code></pre></p><p>If there are issues with the .webp images appearing on the page, make sure that the image/webp MIME type is enabled on your server.</p><p>Apache: add the following code to your .htaccess file:<pre><code>AddType image/webp .webp
</code></pre></p><p>Nginx: add the following code to your mime.types file:<pre><code>image/webp webp;
</code></pre><aside class="note"><b>Note:</b> Vincent Orback has a sample <a href="https://github.com/vincentorback/WebP-images-with-htaccess">htaccess config</a> for serving WebP for reference and Ilya Grigorik maintains a collection of <a href="https://github.com/igrigorik/webp-detect">configuration scripts for serving WebP</a> that can be useful.</aside></p><p><strong>Using the <code>&lt;picture&gt;</code> Tag</strong></p><p>The browser itself is capable of choosing which image format to display through the use of the <code>&lt;picture&gt;</code> tag. The <code>&lt;picture&gt;</code> tag utilizes multiple <code>&lt;source&gt;</code> elements, with one <code>&lt;img&gt;</code> tag, which is the actual DOM element which contains the image. The browser cycles through the sources and retrieves the first match. If the <code>&lt;picture&gt;</code> tag isn’t supported in the user’s browser, a <code>&lt;div&gt;</code> is rendered and the <code>&lt;img&gt;</code> tag is used.<aside class="note"><b>Note:</b> Be careful with the position of <code>&lt;source&gt;</code> as order matters. Don’t place image/webp sources after legacy formats, but instead put them before. Browsers that understand it will use them and those that don’t will move onto more widely supported frameworks. You can also place your images in order of file size if they’re all the same physical size (when not using the <code>media</code> attribute). Generally this is the same order as putting legacy last.</aside></p><p>Here is some sample HTML:<pre><code class="lang-html">&lt;picture&gt;
  &lt;source srcset=&quot;/path/to/image.webp&quot; type=&quot;image/webp&quot;&gt;
  &lt;img src=&quot;/path/to/image.jpg&quot; alt=&quot;&quot;&gt;
&lt;/picture&gt;

&lt;picture&gt;   
    &lt;source srcset=&#39;paul_irish.jxr&#39; type=&#39;image/vnd.ms-photo&#39;&gt;  
    &lt;source srcset=&#39;paul_irish.jp2&#39; type=&#39;image/jp2&#39;&gt;
    &lt;source srcset=&#39;paul_irish.webp&#39; type=&#39;image/webp&#39;&gt;
    &lt;img src=&#39;paul_irish.jpg&#39; alt=&#39;paul&#39;&gt;
&lt;/picture&gt;

&lt;picture&gt;
   &lt;source srcset=&quot;photo.jxr&quot; type=&quot;image/vnd.ms-photo&quot;&gt;
   &lt;source srcset=&quot;photo.jp2&quot; type=&quot;image/jp2&quot;&gt;
   &lt;source srcset=&quot;photo.webp&quot; type=&quot;image/webp&quot;&gt;
   &lt;img src=&quot;photo.jpg&quot; alt=&quot;My beautiful face&quot;&gt;
&lt;/picture&gt;
</code></pre></p><p><strong>Automatic CDN conversion to WebP</strong></p><p>Some CDNs support automated conversion to WebP and can use client hints to serve up WebP images <a href="http://cloudinary.com/documentation/responsive_images#automating_responsive_images_with_client_hints">whenever possible</a>. Check with your CDN to see if WebP support is included in their service. You may have an easy solution just waiting for you.</p><p><strong>WordPress WebP support</strong></p><p><strong>Jetpack</strong> — Jetpack, a popular WordPress plugin, includes a CDN image service called <a href="https://jetpack.com/support/photon/">Photon</a>. With Photon you get seamless WebP image support. The Photon CDN is included in Jetpack&#39;s free level, so this is a good value and a hands-off implementation. The drawback is that Photon resizes your image, puts a query string in your URL and there is an extra DNS lookup required for each image.</p><p><strong>Cache Enabler and Optimizer</strong> — If you are using WordPress, there is at least one halfway-open source option. The open source plugin <a href="https://wordpress.org/plugins/cache-enabler/">Cache Enabler</a> has a menu checkbox option for caching WebP images to be served if available and the current user’s browser supports them. This makes serving WebP images easy. There is a drawback: Cache Enabler requires the use of a sister program called Optimizer, which has an annual fee. This seems out of character for a genuinely open source solution.</p><p><strong>Short Pixel</strong> — Another option for use with Cache Enabler, also at a cost, is Short Pixel. Short Pixel functions much like Optimizer, described above. You can optimize up to 100 images a month for free.</p><p><strong>Compressing Animated GIFs and why <code>&lt;video&gt;</code> is better</strong></p><p>Animated GIFs continue to enjoy widespread use, despite being a very limited format. Although everything from social networks to popular media sites embed animated GIFs heavily, the format was <em>never</em> designed for video storage or animation. In fact, the <a href="https://www.w3.org/Graphics/GIF/spec-gif89a.txt">GIF89a spec</a> notes ‘the GIF is not intended as a platform for animation’. The <a href="http://gifbrewery.tumblr.com/post/39564982268/can-you-recommend-a-good-length-of-clip-to-keep-gifs">number of colors, number of frames and dimensions</a> all impact animated GIF size. Switching to video offers the largest savings.<figure><picture><source data-srcset="images/book-images/animated-gif-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/animated-gif-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/animated-gif-large.jpg"><img class="lazyload" data-src="images/book-images/animated-gif-large.jpg" alt="Animated GIF vs. Video: a comparison of file sizes at ~equivalent quality for different formats."><noscript><img src="images/book-images/animated-gif-large.jpg"></noscript></picture><figcaption>Animated GIF vs. Video: a comparison of file sizes at ~equivalent quality for different formats.</figcaption></figure></p><p><strong>Delivering the same file as an MP4 video can often shave 80% or more off your file-size.</strong> Not only do GIFs often waste significant bandwidth, but they take longer to load, include fewer colors and generally offer sub-part user experiences. You may have noticed animated GIFs uploaded to Twitter perform better on Twitter than on other websites. <a href="http://mashable.com/2014/06/20/twitter-gifs-mp4/#fiiFE85eQZqW">Animated GIFs on Twitter aren’t actually GIFs</a>. To improve user experience and reduce bandwidth consumption, animated GIFs uploaded to Twitter are actually converted to video. Similarly, <a href="https://thenextweb.com/insider/2014/10/09/imgur-begins-converting-gif-uploads-mp4-videos-new-gifv-format/">Imgur converts GIFs to videos</a> on upload, silently converting it to an MP4 for you.</p><p>Why are GIFs many times larger? Animated GIFs store each frame as a lossless GIF image – yes, lossless. The degraded quality we often experience is due to GIFs being limited to a 256-color palette. The format is often large as it doesn’t consider neighbor frames for compression, unlike video codecs like H.264. An MP4 video stores each key frame as a lossy JPEG, which discards some of the original data to achieve better compression.</p><p><strong>If you can switch to videos</strong><ul><li>Use <a href="https://www.ffmpeg.org/">ffmpeg</a> to convert your animated GIFs (or sources) to H.264 MP4s. I use this one-liner from<a href="http://rigor.com/blog/2015/12/optimizing-animated-gifs-with-html5-video"> Rigor</a>: ffmpeg -i animated.gif -movflags faststart -pix_fmt yuv420p -vf &quot;scale=trunc(iw/2)<em>2:trunc(ih/2)</em>2&quot; video.mp4</li><li>ImageOptim API also supports <a href="https://imageoptim.com/api/ungif">converting animated gifs to WebM/H.264 video</a>, <a href="https://github.com/pornel/undither#examples">removing dithering from GIFs</a> which can help video codecs compress even more.</li></ul></p><p><strong>If you must use animated GIFs</strong><ul><li>Tools like Gifsicle can strip metadata, unused palette entries and minimize what changes between frames</li><li>Consider a lossy GIF encoder. The <a href="https://github.com/pornel/giflossy">Giflossy</a> fork of Gifsicle supports this with the <code>—lossy</code> flag and can shave ~60-65% off size. There’s also a nice tool based on it called <a href="https://github.com/vvo/gifify">Gifify</a>. For non-animated GIFs, convert them to PNG or WebP.</li></ul></p><p>For more information, checkout the<a href="https://rigor.com/wp-content/uploads/2017/03/TheBookofGIFPDF.pdf"> Book of GIF</a> by Rigor.</p><h2 id="-a-id-svg-optimization-href-svg-optimization-svg-optimization-a-"><a id="svg-optimization" href="#svg-optimization">SVG Optimization</a></h2><p>Keeping SVGs lean means stripping out anything unnecessary. SVG files created with editors usually contain a large quantity of redundant information (metadata, comments, hidden layers and so forth). This content can often be safely removed or converted to a more minimal form without impacting the final SVG that’s being rendered.<figure><picture><source data-srcset="images/book-images/Modern-Image26-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image26-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image26-large.jpg"><img class="lazyload small" data-src="images/book-images/Modern-Image26-large.jpg" alt="svgo"><noscript><img src="images/book-images/Modern-Image26-large.jpg"></noscript></picture><figcaption><a href="https://jakearchibald.github.io/svgomg/">SVGOMG</a>, by Jake Archibald, is a GUI interface enabling you to optimize your SVGs to your preference by selecting optimizations, with a live preview of the outputted markup</figcaption></figure></p><p><strong>Some general rules for SVG optimization (SVGO):</strong><ul><li>Minify and gzip your SVG files. SVGs are really just text assets expressed in XML, like CSS, HTML and JavaScript, and should be minified and gzipped to improve performance.</li><li>Instead of paths, use predefined SVG shapes like <code>&lt;rect&gt;</code>, <code>&lt;circle&gt;</code>, <code>&lt;ellipse&gt;</code>, <code>&lt;line&gt;</code> and <code>&lt;polygon&gt;</code>. Preferring predefined shapes decreases how much markup is needed to produce a final image, meaning less code to parse and rasterize by the browser. Reducing SVG complexity means a browser can display it more quickly.</li><li>If you must use paths, try to reduce your curves and paths. Simplify and combine them where you can. Illustrator’s <a href="http://jlwagner.net/talks/these-images/#/2/10">simplify tool</a> is adept at removing superfluous points in even complex artwork while smoothing out irregularities.</li><li>Avoid using groups. If you can’t, try to simplify them.</li><li>Delete layers that are invisible.</li><li>Avoid any Photoshop or Illustrator effects. They can get converted to large raster images.</li><li>Double check for any embedded raster images that aren’t SVG-friendly</li><li>Use a tool to optimize your SVGs. <a href="https://jakearchibald.github.io/svgomg/">SVGOMG</a> is a super handy web-based GUI for <a href="https://github.com/svg/svgo">SVGO</a> by Jake Archibald that I’ve found invaluable. If you use Sketch, the <a href="https://www.sketchapp.com/extensions/plugins/svgo-compressor/">Sketch plugin for running SVGO</a> can be used when exporting to shrink the file size.</li></ul><figure><picture><source data-srcset="images/book-images/svgo-precision-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/svgo-precision-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/svgo-precision-large.jpg"><img class="lazyload small" data-src="images/book-images/svgo-precision-large.jpg" alt="svgo precision reduction can sometimes have a positive impact on size"><noscript><img src="images/book-images/svgo-precision-large.jpg"></noscript></picture><figcaption>An example of running an SVG source through SVGO in high-precision mode (leading to a 29% improvement in size) vs. low-precision mode (a 38% size improvement).</figcaption></figure></p><p><a href="https://github.com/svg/svgo">SVGO</a> is a Node-based tool for optimizing SVG. SVGO can reduce file-size by lowering the <em>precision</em> of numbers in your<path></path>definitions. Each digit after a point adds a byte and this is why changing the precision (number of digits) can heavily influence file size. Be very very careful with changing precision however as it can visually impact how your shapes look.<figure><picture><source data-srcset="images/book-images/Modern-Image28-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image28-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image28-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image28-large.jpg" alt="where svgo can go wrong, oversimplifying paths and artwork"><noscript><img src="images/book-images/Modern-Image28-large.jpg"></noscript></picture><figcaption>It’s important to note that while SVGO does well in the previous example without over-simplifying paths and shapes, there are plenty of cases where this may not be the case. Observe how the light strip on the above rocket is distorted at a lower precision.</figcaption></figure></p><p><strong>Using SVGO at the command-line:</strong></p><p>SVGO can be installed as a <a href="https://www.npmjs.com/package/svgo">global npm CLI</a> should you prefer that to a GUI:<pre><code>npm i -g svgo
</code></pre></p><p>This can then be run against a local SVG file as follows:<pre><code>svgo input.svg -o output.svg
</code></pre></p><p>It supports all the options you might expect, including adjusting floating point precision:<pre><code>svgo input.svg --precision=1 -o output.svg
</code></pre></p><p>See the SVGO <a href="https://github.com/svg/svgo">readme</a> for the full list of supported options.</p><p><strong>Don’t forget to compress SVGs!</strong><figure><picture><source data-srcset="images/book-images/before-after-svgo-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/before-after-svgo-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/before-after-svgo-large.jpg"><img class="lazyload" data-src="images/book-images/before-after-svgo-large.jpg" alt="before and after running an image through svgo"><noscript><img src="images/book-images/before-after-svgo-large.jpg"></noscript></picture></figure></p><p>Also, don’t forget to <a href="https://calendar.perfplanet.com/2014/tips-for-optimising-svg-delivery-for-the-web/">Gzip your SVG assets</a> or serve them using Brotli. As they’re text based, they’ll compress really well (~50% of the original sources).</p><p>When Google shipped a new logo, we announced that the <a href="https://twitter.com/addyosmani/status/638753485555671040">smallest</a> version of it was only 305 bytes in size.<figure><picture><source data-srcset="images/book-images/Modern-Image30-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image30-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image30-large.jpg"><img class="lazyload very-small" data-src="images/book-images/Modern-Image30-large.jpg" alt="the smallest version of the new google logo was only 305 bytes in size"><noscript><img src="images/book-images/Modern-Image30-large.jpg"></noscript></picture></figure></p><p>There are <a href="https://www.clicktorelease.com/blog/svg-google-logo-in-305-bytes/">lots of advanced SVG tricks</a> you can use to trim this down even further (all the way to 146 bytes)! Suffice to say, whether it’s through tools or manual clean-up, there’s probably a <em>little</em> more you can shave off your SVGs.</p><p><strong>SVG Sprites</strong></p><p>SVG can be <a href="https://css-tricks.com/icon-fonts-vs-svg/">powerful</a> for icons, offering a way to represent visualizations as a sprite without the <a href="https://www.filamentgroup.com/lab/bulletproof_icon_fonts.html">quirky</a> workarounds needed for icon fonts. It has more granular CSS styling control than icon fonts (SVG stroke properties), better positioning control (no need to hack around pseudo-elements and CSS <code>display</code>) and SVGs are much more <a href="http://www.sitepoint.com/tips-accessible-svg/">accessible</a>.</p><p>Tools like <a href="https://github.com/jkphl/svg-sprite">svg-sprite</a> and <a href="https://icomoon.io/">IcoMoon</a> can automate combining SVGs into sprites which can be used via a <a href="https://css-tricks.com/css-sprites/">CSS Sprite</a>, <a href="https://css-tricks.com/svg-use-with-external-reference-take-2">Symbol Sprite</a> or <a href="http://simurai.com/blog/2012/04/02/svg-stacks">Stacked Sprite</a>. Una Kravetz has a practical <a href="https://una.im/svg-icons/#💁">write-up</a> on how to use gulp-svg-sprite for an SVG sprite workflow worth checking out. Sara Soudein also covers <a href="https://www.sarasoueidan.com/blog/icon-fonts-to-svg/">making the transition from icon fonts to SVG</a> on her blog.</p><p><strong>Further reading</strong></p><p>Sara Soueidan’s <a href="https://calendar.perfplanet.com/2014/tips-for-optimising-svg-delivery-for-the-web/">tips for optimising SVG delivery for the web</a> and Chris Coyier’s <a href="https://abookapart.com/products/practical-svg">Practical SVG book</a> are excellent. I’ve also found Andreas Larsen’s optimizing SVG posts enlightening (<a href="https://medium.com/larsenwork-andreas-larsen/optimising-svgs-for-web-use-part-1-67e8f2d4035">part 1</a>,<a href="https://medium.com/larsenwork-andreas-larsen/optimising-svgs-for-web-use-part-2-6711cc15df46">part 2</a>).<a href="https://medium.com/sketch-app-sources/preparing-and-exporting-svg-icons-in-sketch-1a3d65b239bb">Preparing and exporting SVG icons in Sketch</a> was also a great read.</p><h2 id="-a-id-avoid-recompressing-images-lossy-codecs-href-avoid-recompressing-images-lossy-codecs-avoid-recompressing-images-with-lossy-codecs-a-"><a id="avoid-recompressing-images-lossy-codecs" href="#avoid-recompressing-images-lossy-codecs">Avoid recompressing images with lossy codecs</a></h2><p>It is recommended to always compress from the original image. Recompressing images has consequences. Let’s say you take a JPEG that’s already been compressed with a quality of 60. If you recompress this image with lossy encoding, it will look worse. Each additional round of compression is going to introduce generational loss – information will be lost and compression artifacts will start to build up. Even if you’re re-compressing at a high quality setting.</p><p>To avoid this trap, <strong>set the lowest good quality you’re willing to accept in the first place</strong> and you’ll get maximum file savings from the start. You then avoid this trap because any file-size reductions from quality reduction alone will look bad.</p><p>Re-encoding a lossy file will almost always give you a smaller file, but this doesn’t mean you’re getting as much quality out of it as you may think.<figure><picture><source data-srcset="images/book-images/generational-loss-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/generational-loss-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/generational-loss-large.jpg"><img class="lazyload" data-src="images/book-images/generational-loss-large.jpg" alt="generational loss when re-encoding an image multiple times"><noscript><img src="images/book-images/generational-loss-large.jpg"></noscript></picture><figcaption>Above, from this <a href="https://www.youtube.com/watch?v=w7vXJbLhTyI">excellent video</a> and <a href="http://cloudinary.com/blog/why_jpeg_is_like_a_photocopier">accompanying article</a> by Jon Sneyers, we can see the generational loss impact of recompression using several formats. This is a problem you may have run into if saving (already compressed) images from social networks and re-uploading them (causing recompression). Quality loss will build up.</figcaption></figure></p><p>MozJPEG (perhaps accidentally) has a better resistance to recompression degradation thanks to trellis quantization. Instead of compressing all DCT values as they are exactly, it can check close values within a +1/-1 range to see if similar values compress to fewer bits. Lossy FLIF has a hack similar to lossy PNG in that prior to (re)compression, it can look at the data and decide what to throw away. Recompressed PNGs have ‘holes’ it can detect to avoid changing data further.</p><p><strong>When editing your source files, store them in a lossless format like PNG or TIFF, so you preserve as much quality as you can.</strong> Your build tools or image compression service than then handle outputting the compressed version you serve to users with minimal loss in quality.</p><h2 id="-a-id-reduce-unnecessary-image-decode-costs-href-reduce-unnecessary-image-decode-costs-reduce-unnecessary-image-decode-and-resize-costs-a-"><a id="reduce-unnecessary-image-decode-costs" href="#reduce-unnecessary-image-decode-costs">Reduce unnecessary image decode and resize costs</a></h2><p>We’ve all shipped large, higher resolution images than needed to our users before. This has a cost to it. Decoding and resizing images are expensive operations for a browser on average mobile hardware. If sending down large images and rescaling using CSS or width/height attributes, you’re likely to see this happen and it can impact performance.<figure><picture><source data-srcset="images/book-images/image-pipeline-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/image-pipeline-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/image-pipeline-large.jpg"><img class="lazyload" data-src="images/book-images/image-pipeline-large.jpg" alt="There are many steps involved in a browser grabbing an image specified in a tag and displaying it on a screen. These include request, decode, resize, copy to GPU and display."><noscript><img src="images/book-images/image-pipeline-large.jpg"></noscript></picture><figcaption>When a browser fetches an image, it has to decode the image from the original source format (e.g. JPEG) to a bitmap in memory. Often the image needs to be resized (e.g. width has been set to a percentage of its container). Decoding and resizing images are expensive and can delay how long it takes for an image to be displayed.</figcaption></figure></p><p>Sending down images that a browser can render without needing to resize at all is ideal. So, serve the smallest images for your target screen sizes and resolutions, taking advantage of <a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images"><code>srcset</code> and <code>sizes</code></a> – we’ll cover <code>srcset</code> shortly.</p><p>Omitting the <code>width</code> or <code>height</code> attributes on an image can also negatively impact performance. Without them, a browser assigns a smaller placeholder region for the image until sufficient bytes have arrived for it to know the correct dimensions. At that point, the document layout must be updated in what can be a costly step called reflow.<figure><picture><source data-srcset="images/book-images/devtools-decode-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/devtools-decode-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/devtools-decode-large.jpg"><img class="lazyload small" data-src="images/book-images/devtools-decode-large.jpg" alt="image decode costs shown in the chrome devtools"><noscript><img src="images/book-images/devtools-decode-large.jpg"></noscript></picture><figcaption>Browsers have to go through a number of steps to paint images on the screen. In addition to fetching them, images need to be decoded and often resized. These events can be audited in the Chrome DevTools <a href="https://developers.google.com/web/tools/chrome-devtools/evaluate-performance/performance-reference">Timeline</a>.</figcaption></figure></p><p>Larger images also come with an increase in memory size costs. Decoded images are ~4 bytes per pixel. If you’re not careful, you can literally crash the browser; on low-end devices it doesn’t take that much to start memory swapping. So, keep an eye on your image decode, resize and memory costs.<figure><picture><source data-srcset="images/book-images/image-decoding-mobile-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/image-decoding-mobile-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/image-decoding-mobile-large.jpg"><img class="lazyload small" data-src="images/book-images/image-decoding-mobile-large.jpg" alt="Decoding images can be incredibly costly on average and lower-end mobile hardware"><noscript><img src="images/book-images/image-decoding-mobile-large.jpg"></noscript></picture><figcaption>Decoding images can be incredibly costly on average and lower-end mobile phones. In some cases it can be 5× slower to decode (if not longer).</figcaption></figure></p><p>When building their new <a href="https://medium.com/@paularmstrong/twitter-lite-and-high-performance-react-progressive-web-apps-at-scale-d28a00e780a3">mobile web experience</a>, Twitter improved image decode performance by ensuring they served appropriately sized images to their users. This took decode time for many images in the Twitter timeline from ~400ms all the way down to ~19!<figure><picture><source data-srcset="images/book-images/image-decoding-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/image-decoding-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/image-decoding-large.jpg"><img class="lazyload" data-src="images/book-images/image-decoding-large.jpg" alt="Chrome DevTools Timeline/Performance panel highlighting image decode times before and after Twitter Lite optimized their image pipeline. Before was higher."><noscript><img src="images/book-images/image-decoding-large.jpg"></noscript></picture><figcaption>Chrome DevTools Timeline/Performance panel highlighting image decode times (in green) before and after Twitter Lite optimized their image pipeline.</figcaption></figure></p><h3 id="-a-id-delivering-hidpi-with-srcset-href-delivering-hidpi-with-srcset-delivering-hidpi-images-using-srcset-a-"><a id="delivering-hidpi-with-srcset" href="#delivering-hidpi-with-srcset">Delivering HiDPI images using <code>srcset</code></a></h3><p>Users may access your site through a range of mobile and desktop devices with high-resolution screens. The <a href="https://stackoverflow.com/a/21413366">Device Pixel Ratio</a> (DPR) (also called the ‘CSS pixel ratio’) determines how a device’s screen resolution is interpreted by CSS. DPR was created by phone manufacturers to enable increasing the resolution and sharpness of mobile screens without making elements appear too small.</p><p>To match the image quality users might expect, deliver the most appropriate resolution images to their devices. Sharp, high-DPR images (e.g. 2×, 3×) can be served to devices that support them. Low and standard-DPR images should be served to users without high-res screens as such 2×+ images will often weigh significantly more bytes.<figure><picture><source data-srcset="images/book-images/device-pixel-ratio-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/device-pixel-ratio-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/device-pixel-ratio-large.jpg"><img class="lazyload" data-src="images/book-images/device-pixel-ratio-large.jpg" alt="A diagram of the device pixel ratio at 1×, 2× and 3×. Image quality appears to sharpen
        as DPR increases and a visual is shown comparing device pixels to CSS pixels."><noscript><img src="images/book-images/device-pixel-ratio-large.jpg"></noscript></picture><figcaption>Device Pixel Ratio: Many sites track the DPR for popular devices including <a href="https://material.io/devices/">material.io</a> and <a href="https://mydevice.io/devices/">mydevice.io</a>.</figcaption></figure></p><p><a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images">srcset</a> allows a browser to select the best available image per device, e.g. selecting a 2× image for a 2× mobile display. Browsers without <code>srcset</code> support can fallback to the default <code>src</code> specified in the <code>&lt;img&gt;</code> tag.<pre><code>&lt;img srcset=&quot;paul-irish-320w.jpg,
             paul-irish-640w.jpg 2x,
             paul-irish-960w.jpg 3x&quot;
     src=&quot;paul-irish-960w.jpg&quot; alt=&quot;Paul Irish cameo&quot;&gt;
</code></pre></p><p>Image CDNs like <a href="http://cloudinary.com/blog/how_to_automatically_adapt_website_images_to_retina_and_hidpi_devices">Cloudinary</a> and <a href="https://docs.imgix.com/apis/url/dpr">Imgix</a> both support controlling image density to serve the best density to users from a single canonical source.<aside class="note"><b>Note:</b> You can learn more about Device Pixel Ratio and responsive images in this free <a href="https://www.udacity.com/course/responsive-images--ud882">Udacity</a> course and the <a href="https://developers.google.com/web/fundamentals/design-and-ui/responsive/images">Images</a> guide on Web Fundamentals.</aside></p><p>A friendly reminder that <a href="https://www.smashingmagazine.com/2016/01/leaner-responsive-images-client-hints/">Client Hints</a> can also provide an alternative to specifying each possible pixel density and format in your responsive image markup. Instead, they append this information to the HTTP request so web servers can pick the best fit for the current device’s screen density.</p><h3 id="-a-id-art-direction-href-art-direction-art-direction-a-"><a id="art-direction" href="#art-direction">Art direction</a></h3><p>Although shipping the right resolution to users is important, some sites also need to think about this in terms of <strong><a href="http://usecases.responsiveimages.org/#art-direction">art direction</a></strong>. If a user is on a smaller screen, you may want to crop or zoom in and display the subject to make best use of available space. Although art direction is outside the scope of this write-up, services like<a href="http://cloudinary.com/blog/automatically_art_directed_responsive_images%20"> Cloudinary</a> provide APIs to try automating this as much as possible.<figure><picture><source data-srcset="images/book-images/responsive-art-direction-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/responsive-art-direction-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/responsive-art-direction-large.jpg"><img class="lazyload" data-src="images/book-images/responsive-art-direction-large.jpg" alt="responsive art direction in action, adapting to show more or less of an image in a cropped manner depending on device"><noscript><img src="images/book-images/responsive-art-direction-large.jpg"></noscript></picture><figcaption>Art direction: Eric Portis put together an excellent <a href="https://ericportis.com/etc/cloudinary/">sample</a> of how responsive images can be used for art-direction. This example adapts the main hero image’s visual characteristics at different breakpoints to make best use of the available space.</figcaption></figure></p><h2 id="-a-id-color-management-href-color-management-color-management-a-"><a id="color-management" href="#color-management">Color management</a></h2><p>There are at least three different perspectives of color: biology, physics and print. In biology, color is a <a href="http://hubel.med.harvard.edu/book/ch8.pdf">perceptual phenomenon</a>. Objects reflect light in different combinations of wavelengths. Light receptors in our eyes translate these wavelengths into the sensation we know as color. In physics, it’s light that matters – light frequencies and brightness. Print is more about color wheels, inks and artistic models.</p><p>Ideally, every screen and web browser in the world would display color exactly the same. Unfortunately, due to a number of inherent inconsistencies, they don’t. Color management allows us to reach a compromise on displaying color through color models, spaces and profiles.</p><h4 id="color-models">Color models</h4><p><a href="https://en.wikipedia.org/wiki/Gamma_correction">Color models</a> are a system for generating a complete range of colors from a smaller set of primary colors. There are different types of color spaces which use different parameters to control colors. Some color spaces have fewer control parameters than others – e.g. grayscale only has a single parameter for controlling brightness between black and white colors.</p><p>Two common color models are additive and subtractive. Additive color models (like RGB, used for digital displays) use light to show color while subtractive color models (like CMYK, used in printing) work by taking light away.<figure><picture><source data-srcset="images/book-images/colors_ept6f2-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/colors_ept6f2-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/colors_ept6f2-large.jpg"><img class="lazyload small" data-src="images/book-images/colors_ept6f2-large.jpg" alt="sRGB, Adobe RGB and ProPhoto RGB"><noscript><img src="images/book-images/colors_ept6f2-small.jpg"></noscript></picture><figcaption>In RGB red, green and blue light are added in different combinations to produce a broad spectrum of colors. CYMK (cyan, magenta, yellow and black) works through different colors of ink subtracting brightness from white paper.</figcaption></figure></p><p><a href="https://www.designersinsights.com/designer-resources/understanding-color-models/">Understanding Color Models and Spot Color Systems</a> has a good description of other color models and modes, such as HSL, HSV and LAB.</p><h4 id="color-spaces">Color spaces</h4><p><a href="http://www.dpbestflow.org/color/color-space-and-color-profiles#space">Color spaces</a> are a specific range of colors that can be represented for a given image. For example, if an image contains up to 16.7 million colors, different color spaces allow the use of narrower or wider ranges of these colors. Some developers refer to color models and color spaces as the same thing.</p><p><a href="https://en.wikipedia.org/wiki/SRGB">sRGB</a> was designed to be a <a href="https://www.w3.org/Graphics/Color/sRGB.html">standard</a> color space for the web and is based on RGB. It’s a small color space that is typically considered the lowest common denominator and is the safest option for color management cross-browser. Other color spaces (such as <a href="https://en.wikipedia.org/wiki/Adobe_RGB_color_space">Adobe RGB</a> or <a href="https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space">ProPhoto RGB</a> – used in Photoshop and Lightroom) can represent more vibrant colors than sRGB but as the latter is more ubiquitous across most web browsers, games and monitors, it’s what is generally focused on.<figure><picture><source data-srcset="images/book-images/color-wheel_hazsbk-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/color-wheel_hazsbk-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/color-wheel_hazsbk-large.jpg"><img class="lazyload small" data-src="images/book-images/color-wheel_hazsbk-large.jpg" alt="sRGB, Adobe RGB and ProPhoto RGB"><noscript><img src="images/book-images/color-wheel_hazsbk-small.jpg"></noscript></picture><figcaption>Above we can see a visualization of gamut – the range of colors a color space can define.</figcaption></figure></p><p>Color spaces have three channels (red, green and blue). There are 255 colors possible in each channel under 8-bit mode, bringing us to a total of 16.7 million colors. 16-bit images can show trillions of colors.<figure><picture><source data-srcset="images/book-images/srgb-rgb_ntuhi4-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/srgb-rgb_ntuhi4-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/srgb-rgb_ntuhi4-large.jpg"><img class="lazyload small" data-src="images/book-images/srgb-rgb_ntuhi4-large.jpg" alt="sRGB, Adobe RGB and ProPhoto RGB"><noscript><img src="images/book-images/srgb-rgb_ntuhi4-small.jpg"></noscript></picture><figcaption>A comparison of sRGB, Adobe RGB and ProPhoto RGB using an image from <a href="https://yardstick.pictures/tags/img%3Adci-p3">Yardstick</a>. It’s incredibly hard to show this concept in sRGB, when you can’t show colors that can’t be seen. A regular photo in sRGB vs wide gamut should have everything identical, except most saturated ‘juicy’ colors.</figcaption></figure></p><p>The differences in color spaces (like sRGB, Adobe RGB and ProPhoto RGB) are their gamut (the range of colors they can reproduce with shades), illuminant and <a href="http://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/">gamma</a> curves. sRGB is ~20% smaller than Adobe RGB and ProPhoto RGB is ~<a href="http://www.petrvodnakphotography.com/Articles/ColorSpace.htm">50% larger</a> than Adobe RGB. The above image sources are from <a href="http://clippingpathzone.com/blog/essential-photoshop-color-settings-for-photographers">Clipping Path</a>.</p><p><a href="http://www.astramael.com/">Wide-gamut</a> is a term describing color spaces with a gamut larger than sRGB. These types of displays are becoming more common. That said, many digital displays are still simply unable to display color profiles that are significantly better than sRGB. When saving for the web in Photoshop, consider using the ‘Convert to sRGB’ option unless targeting users with higher-end wide-gamut screens.<aside class="key-point"><b>Note:</b> When working with original photography, avoid using sRGB as your primary color space. It’s smaller than the color spaces most cameras support and can cause clipping. Instead, work on a larger color space (like ProPhoto RGB) and output to sRGB when exporting for the web.</aside></p><p><strong>Are there any cases where wide gamut makes sense for web content?</strong></p><p>Yes. If an image contains very saturated/juicy/vibrant color and you care about it being just as juicy on screens that support it. However, in real photos that rarely happens. Often it’s easy to tweak color to make it appear vibrant, without it actually exceeding sRGB gamut</p><p>That’s because human color perception is not absolute, but relative to our surroundings and is easily fooled. If your image contains a fluorescent highlighter color, then you’ll have an easier time with wide gamut.</p><h4 id="gamma-correction-and-compression">Gamma correction and compression</h4><p><a href="https://en.wikipedia.org/wiki/Gamma_correction">Gamma correction</a> (or just Gamma) controls the overall brightness of an image. Changing the gamma can also alter the ratio of red to green and blue colors. Images without gamma correction can look like their colors are bleached out or too dark.</p><p>In video and computer graphics, gamma is used for compression, similar to data compression. This allows you to squeeze useful levels of brightness in fewer bits (8-bit rather than 12 or 16). Human perception of brightness is not linearly proportional to physical amount of light. Representing colors in their true physical form would be wasteful when encoding images for human eyes. Gamma compression is used to encode brightness on a scale that is closer to human perception.</p><p>With gamma compression useful scale of brightness fits in 8 bits of precision (0-255 used by most RGB colors). All of this comes from the fact that if colors used some unit with 1:1 relationship to physics, RGB values would be from 1 to million where values 0-1000 would look distinct, but values between 999000-1000000 would look identical. Imagine being in a dark room where there is just 1 candle. Light a second candle and you notice significant increases in brightness in the room light. Add a third candle and it’ll seem even brighter. Now imagine being in a room with 100 candles. Light the 101st candle, the 102nd. You won’t notice a change in brightness.</p><p>Even though in both cases, physically, exactly the same amount of light was added. So because eyes are less sensitive when light is bright, gamma compression ‘compresses’ bright values, so in physical terms bright levels are less precise but the scale is adjusted for humans so from the human perspective all values are equally precise.<aside class="key-point"><b>Note:</b> Gamma compression/correction here is different to the image gamma curves you might configure in Photoshop. When gamma compression works as it should, it doesn’t look like anything.</aside></p><h4 id="color-profiles">Color profiles</h4><p>A color profile is the information describing what that the color space of a device is. It’s used to convert between different color spaces. Profiles attempt to ensure an image looks as similar as possible on these different kinds of screens and mediums.</p><p>Images can have an embedded color profile as described by the <a href="http://www.color.org/icc_specs2.xalter">International Color Consortium</a> (ICC) to represent precisely how colors should appear. This is supported by different formats including JPEGs, PNGs, SVGs and <a href="https://developers.google.com/speed/webp/docs/riff_container">WebP</a> and most major browsers support embedded ICC profiles. When an image is displayed in an app and it knows the monitor’s capabilities, these colors can be adjusted based on the color profile.<aside class="key-point"><b>Note:</b> Some monitors have a color profile similar to sRGB and cannot display much better profiles so depending on your target users displays, there may be limited value in embedding them. Check who your target users are.</aside></p><p>Embedded color profiles can also heavily increase the size of your images (100KB+ occasionally) so be careful with embedding. Tools like ImageOptim will actually <a href="https://imageoptim.com/color-profiles.html">automatically</a> remove color profiles if it finds them. In contrast, with the ICC profile removed in the name of size reduction, browsers will be forced to display the image in your monitor’s color space which can lead to differences in expected saturation and contrast. Evaluate the trade-offs here make sense for your use case.</p><p><a href="https://ninedegreesbelow.com/photography/articles.html">Nine Degrees Below</a> have an excellent set of resources on ICC profile color management if you are interested in learning more about profiles.</p><h4 id="color-profiles-and-web-browsers">Color profiles and web browsers</h4><p>Earlier versions of Chrome did not have great support for color management, but this is improving in 2017 with <a href="https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/ptuKdRQwPAo">Color Correct Rendering</a>. Displays that are not sRGB (newer MacBook Pros) will convert colors from sRGB to the display profile. This will mean colors should look more similar across different systems and browsers. Safari, Edge and Firefox can now also take ICC profiles into account, so images with a different color profile (e.g. ICC) can now display them correctly whether your screen has wide gamut or not.<aside class="key-point"><b>Note:</b> For a great guide on how color applies to a broader spectrum of ways we work on the web, see the <a href="https://css-tricks.com/nerds-guide-color-web/">nerd’s guide to color on the web</a> by Sarah Drasner.</aside></p><h2 id="-a-id-image-sprites-href-image-sprites-image-spriting-a-"><a id="image-sprites" href="#image-sprites">Image spriting</a></h2><p><a href="https://developers.google.com/web/fundamentals/design-and-ui/responsive/images#use_image_sprites">Image sprites</a> (or CSS sprites) have a long history on the web, are supported by all browsers and have been a popular way to reduce the number of images a page loads by combining them into a single larger image that is sliced.<figure><picture><source data-srcset="images/book-images/i2_2ec824b0_1-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/i2_2ec824b0_1-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/i2_2ec824b0_1-large.jpg"><img class="lazyload small" data-src="images/book-images/i2_2ec824b0_1-large.jpg" alt="Image sprites are still widely used in large, production sites, including the Google homepage."><noscript><img src="images/book-images/i2_2ec824b0_1-large.jpg"></noscript></picture><figcaption>Image sprites are still widely used in large, production sites, including the Google homepage.</figcaption></figure></p><p>Under HTTP/1.x, some developers used spriting to reduce HTTP requests. This came with a number of benefits, however care was needed as you quickly ran into challenges with cache-invalidation – changes to any small part of an image sprite would invalidate the entire image in a user’s cache.</p><p>Spriting may now however be an <a href="https://hpbn.co/http2/">HTTP/2</a> anti-pattern. With HTTP/2, it may be best to <a href="https://deliciousbrains.com/performance-best-practices-http2/">load individual images</a> since multiple requests within a single connection are now possible. Measure to evaluate whether this is the case for your own network setup.</p><h2 id="-a-id-lazy-load-non-critical-images-href-lazy-load-non-critical-images-lazy-load-non-critical-images-a-"><a id="lazy-load-non-critical-images" href="#lazy-load-non-critical-images">Lazy-load non-critical images</a></h2><p>Lazy loading is a web performance pattern that delays the loading of images in the browser until the user needs to see it. One example is, as you scroll, images load asynchronously on demand. This can further compliment the byte-savings you see from having an image compression strategy.<figure><picture><source data-srcset="images/book-images/scrolling-viewport-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/scrolling-viewport-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/scrolling-viewport-large.jpg"><img class="lazyload" data-src="images/book-images/scrolling-viewport-large.jpg" alt="lazy-loading images"><noscript><img src="images/book-images/scrolling-viewport-large.jpg"></noscript></picture></figure></p><p>Images that must appear ‘above the fold’, or when the web page first appears are loaded straight away. The images which follow ‘below the fold’, however, are not yet visible to the user. They do not have to be immediately loaded into the browser. They can be loaded later — or lazy loaded — only if and when the user scrolls down and it becomes necessary to show them.</p><p>Lazy loading is not yet natively supported in the browser itself (although there have been <a href="https://discourse.wicg.io/t/a-standard-way-to-lazy-load-images/1153/10">discussions</a> about it in the past). Instead, we use JavaScript to add this capability.</p><p><strong>Why is Lazy Loading Useful?</strong></p><p>This ‘lazy’ way of loading images only if and when necessary has many benefits:<ul><li><strong>Reduced data consumption</strong>: As you aren’t assuming the user will need every image fetched ahead of time, you’re only loading the minimal number of resources. This is always a good thing, especially on mobile with more restrictive data plans.</li><li><strong>Reduced battery consumption</strong>: Less workload for the user’s browser which can save on battery life.</li><li><strong>Improved download speed</strong>: Decreasing your overall page load time on an image heavy website from several seconds to almost nothing is a tremendous boost to user experience. In fact, it could be the difference between a user staying around to enjoy your site and just another bounce statistic.</li></ul></p><p><strong>But like all tools, with great power comes great responsibility.</strong></p><p><strong>Avoid lazy-loading images above the fold.</strong> Use it for long-lists of images (e.g. products) or lists of user avatars. Don’t use it for the main page hero image. Lazy-loading images above the fold can make loading visibly slower, both technically and for human perception. It can kill the browser’s preloader, progressive loading and the JavaScript can create extra work for the browser.</p><p><strong>Be very careful lazy-loading images when scrolling.</strong> If you wait until the user is scrolling they are likely to see placeholders and may eventually get images, if they haven’t already scrolled past them. One recommendation would be to start lazy-loading after the above-the-fold images have loaded, loading all of the images independent of user interaction.</p><p><strong>Who Uses Lazy Loading?</strong></p><p>For examples of lazy loading, look at most any major site that hosts a lot of images. Some notable sites are <a href="https://medium.com/">Medium</a> and <a href="https://www.pinterest.com/">Pinterest</a>.<figure><picture><source data-srcset="images/book-images/Modern-Image35-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image35-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image35-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image35-large.jpg" alt="inline previews for images on medium.com"><noscript><img src="images/book-images/Modern-Image35-large.jpg"></noscript></picture><figcaption>An example of Gaussian-blurred inline previews for images on Medium.com</figcaption></figure></p><p>A number of sites (such as Medium) display a small, Gaussian-blurred inline preview (a few 100 bytes) that transitions (lazy-loads) to a full-quality image once it has been fetched.</p><p>José M. Pérez has written about how to implement the Medium effect using <a href="https://jmperezperez.com/medium-image-progressive-loading-placeholder/">CSS filters</a> and experimented with <a href="https://jmperezperez.com/webp-placeholder-images/">different image formats</a> to support such placeholders. Facebook also did a write-up on their famous 200-byte approach for such placeholders for their <a href="https://code.facebook.com/posts/991252547593574/the-technology-behind-preview-photos/">cover photos</a> that is worth a read. If you’re a Webpack user, <a href="https://lqip-loader.firebaseapp.com/">LQIP loader</a> can help automate some of this work away.</p><p>In fact, you can search for your favorite source of high-res photos and then scroll down the page. In almost all cases you’ll experience how the website loads only a few full-resolution images at a time, with the rest being placeholder colors or images. As you continue to scroll, the placeholder images are replaced with full-resolution images. This is lazy loading in action.</p><p>A technique that has been making the rounds recently is <em>vector-</em> rather than pixel-based low-quality image previews, piloted by Tobias Baldauf in his tool <a href="https://github.com/technopagan/sqip">SQIP</a>. This approach makes use of the utility <a href="https://github.com/fogleman/primitive">Primitive</a> to generate an SVG preview consisting of several simple shapes that approximate the main features visible inside the target image, optimizes the SVG using <a href="https://github.com/svg/svgo">SVGO</a>, and finally adds a Gaussian Blur filter to it; producing an SVG placeholder that weighs in at only ~800–1000 bytes, looks crisp on all screens, and provides a visual cue of the image contents to come. Both, lazy-loading and low-quality image previews, can obviously <a href="https://calendar.perfplanet.com/2017/progressive-image-loading-using-intersection-observer-and-sqip/">be combined</a>.</p><p><strong>How Can I Apply Lazy Loading to My Pages?</strong></p><p>There are a number of techniques and plugins available for lazy loading. I recommend <a href="https://github.com/aFarkas/lazysizes">lazysizes</a> by Alexander Farkas because of its decent performance, features, its optional integration with <a href="https://developers.google.com/web/updates/2016/04/intersectionobserver">Intersection Observer</a>, and support for plugins.</p><p><strong>What Can I Do with Lazysizes?</strong></p><p>Lazysizes is a JavaScript library. It requires no configuration. Download the minified js file and include it in your webpage.</p><p>Here is some example code taken from the README file:</p><p>Add the class ‘lazyload’ to your images/iframes in conjunction with a data-src and/or data-srcset attribute.</p><p>Optionally you can also add a src attribute with a low quality image:<pre><code class="lang-html">&lt;!-- non-responsive: --&gt;
&lt;img data-src=&quot;image.jpg&quot; class=&quot;lazyload&quot; /&gt;

&lt;!-- responsive example with automatic sizes calculation: --&gt;
&lt;img
    data-sizes=&quot;auto&quot;
    data-src=&quot;image2.jpg&quot;
    data-srcset=&quot;image1.jpg 300w,
    image2.jpg 600w,
    image3.jpg 900w&quot; class=&quot;lazyload&quot; /&gt;

&lt;!-- iframe example --&gt;

&lt;iframe frameborder=&quot;0&quot;
    class=&quot;lazyload&quot;
    allowfullscreen=&quot;&quot;
    data-src=&quot;//www.youtube.com/embed/ZfV-aYdU4uE&quot;&gt;
&lt;/iframe&gt;
</code></pre></p><p>For the web version of this book, I paired Lazysizes (although you can use any alternative) with Cloudinary for on-demand responsive images. This allowed me the freedom to experiment with different values for scale, quality, format and whether or not to progressively load with minimal effort:<figure><picture><source data-srcset="images/book-images/cloudinary-responsive-images-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/cloudinary-responsive-images-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/cloudinary-responsive-images-large.jpg"><img class="lazyload" data-src="images/book-images/cloudinary-responsive-images-large.jpg" alt="Cloudinary supports on-demand control of image quality, format and several other features."><noscript><img src="images/book-images/cloudinary-responsive-images-large.jpg"></noscript></picture></figure></p><p><strong>Lazysizes features include:</strong><ul><li>Automatically detects visibility changes on current and future lazyload elements</li><li>Includes standard responsive image support (picture and srcset)</li><li>Adds automatic sizes calculation and alias names for media queries feature</li><li>Can be used with hundreds of images/iframes on CSS and JS-heavy pages or web apps</li><li>Extendable: Supports plugins</li><li>Lightweight but mature solution</li><li>SEO improved: Does not hide images/assets from crawlers</li></ul></p><p><strong>More Lazy Loading Options</strong></p><p>Lazysizes is not your only option. Here are more lazy loading libraries:<ul><li><a href="http://ressio.github.io/lazy-load-xt/">Lazy Load XT</a></li><li><a href="https://github.com/dinbror/blazy">BLazy.js</a> (or [Be]Lazy)</li><li><a href="http://luis-almeida.github.io/unveil/">Unveil</a></li><li><a href="https://github.com/malchata/yall.js">yall.js (Yet Another Lazy Loader)</a> which is ~1KB and uses Intersection Observer where supported.</li></ul></p><p><strong>What’s the catch with Lazy Loading?</strong><ul><li>Screen readers, some search bots and any users with JavaScript disabled will not be able to view images lazy loaded with JavaScript. This is however something that we can work around with a <code>&lt;noscript&gt;</code> fallback.</li><li>Scroll listeners, such as used for determining when to load a lazy-loaded image, can have an adverse impact on browser scrolling performance. They can cause the browser to redraw many times, slowing the process to a crawl – however, smart lazy loading libraries will use throttling to mitigate this. One possible solution is Intersection Observer, which is supported by lazysizes.</li></ul></p><p>Lazy loading images is a widespread pattern for reducing bandwidth, decreasing costs, and improving user experience. Evaluate whether it makes sense for your experience. For further reading see <a href="https://jmperezperez.com/lazy-loading-images/">lazy loading images</a> and <a href="https://jmperezperez.com/medium-image-progressive-loading-placeholder/">implementing Medium’s progressive loading</a>.</p><h2 id="-a-id-display-none-trap-href-display-none-trap-avoiding-the-display-none-trap-a-"><a id="display-none-trap" href="#display-none-trap">Avoiding the display:none trap</a></h2><p>Older responsive image solutions have mistaken how browsers handle image requests when setting the CSS <code>display</code> property. This can cause significantly more images to be requested than you might be expecting and is another reason <code>&lt;picture&gt;</code> and <code>&lt;img srcset&gt;</code> are preferred for loading responsive images.</p><p>Have you ever written a media query that sets an image to <code>display:none</code> at certain breakpoints?<pre><code class="lang-html">&lt;img src=&quot;img.jpg&quot;&gt;
&lt;style&gt;
@media (max-width: 640px) {
    img {
        display: none;
    }
}
&lt;/style&gt;
</code></pre></p><p>Or toggled what images are hidden using a <code>display:none</code> class?<pre><code class="lang-html">&lt;style&gt;
.hidden {
  display: none;
}
&lt;/style&gt;
&lt;img src=&quot;img.jpg&quot;&gt;
&lt;img src=“img-hidden.jpg&quot; class=&quot;hidden&quot;&gt;
</code></pre></p><p>A quick check against the Chrome DevTools network panel will verify that images hidden using these approaches still get fetched, even when we expect them not to be. This behavior is actually correct per the embedded resources spec.<figure><picture><source data-srcset="images/book-images/display-none-images-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/display-none-images-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/display-none-images-large.jpg"><img class="lazyload" data-src="images/book-images/display-none-images-large.jpg" alt="Images hidden with display:none still get fetched"><noscript><img src="images/book-images/display-none-images-large.jpg"></noscript></picture></figure></p><p><strong>Does <code>display:none</code> avoid triggering a request for an image <code>src</code>?</strong><pre><code class="lang-html">&lt;div style=&quot;display:none&quot;&gt;&lt;img src=&quot;img.jpg&quot;&gt;&lt;/div&gt;
</code></pre></p><p>No. The image specified will still get requested. A library cannot rely on display:none here as the image will be requested before JavaScript can alter the src.</p><p><strong>Does <code>display:none</code> avoid triggering a request for a <code>background: url()</code>?</strong><pre><code class="lang-html">&lt;div style=&quot;display:none&quot;&gt;
  &lt;div style=&quot;background: url(img.jpg)&quot;&gt;&lt;/div&gt;
&lt;/div&gt;
</code></pre></p><p>Yes. CSS backgrounds aren’t fetched as soon as an element is parsed. Calculating CSS styles for children of elements with <code>display:none</code> would be less useful as they don’t impact rendering of the document. Background images on child elements are not calculated nor downloaded.</p><p>Jake Archibald’s <a href="https://jakearchibald.github.io/request-quest/">Request Quest</a> has an excellent quiz on the pitfalls of using <code>display:none</code> for your responsive images loading. When in doubt about how specific browser’s handle image request loading, pop open their DevTools and verify for yourself.</p><p>Again, where possible, use <code>&lt;picture&gt;</code> and <code>&lt;img srcset&gt;</code> instead of relying on <code>display:none</code>.</p><h2 id="-a-id-image-processing-cdns-href-image-processing-cdns-does-an-image-processing-cdn-make-sense-for-you-a-"><a id="image-processing-cdns" href="#image-processing-cdns">Does an image processing CDN make sense for you?</a></h2><p><em>The time you’ll spend reading the blog posts to setup your own image processing pipeline and tweaking your config is often &gt;&gt; the fee for a service. With <a href="http://cloudinary.com/">Cloudinary</a> offering a free service, <a href="https://www.imgix.com/">Imgix</a> a free trial and <a href="https://github.com/thumbor/thumbor">Thumbor</a> existing as an OSS alternative, there are plenty of options available to you for automation.</em></p><p>To achieve optimal page load times, you need to optimize your image loading. This optimization calls for a responsive image strategy and can benefit from on-server image compression, auto-picking the best format and responsive resizing. What matters is that you deliver the correctly sized image to the proper device in the proper resolution as fast as possible. Doing this is not as easy as one might think.</p><p><strong>Using Your Server vs. a CDN</strong></p><p>Because of the complexity and ever-evolving nature of image manipulation, we’re going to offer a quote from someone with experience in the field, then proceed with a suggestion.</p><p>&quot;If your product is not image manipulation, then don’t do this yourself. Services like Cloudinary [or imgix, Ed.] do this much more efficiently and much better than you will, so use them. And if you’re worried about the cost, think about how much it’ll cost you in development and upkeep, as well as hosting, storage, and delivery costs.&quot; — <a href="https://medium.com/@cmgmyr/moving-from-self-hosted-image-service-to-cloudinary-bd7370317a0d">Chris Gmyr</a></p><p>For the moment, we are going to agree and suggest that you consider using a CDN for your image processing needs. Two CDNs will be examined to see how they compare relative to the list of tasks we raised earlier.</p><p><strong>Cloudinary and imgix</strong></p><p><a href="http://cloudinary.com/">Cloudinary</a> and <a href="https://www.imgix.com/">imgix</a> are two established image processing CDNs. They are the choice of hundreds of thousands of developers and companies worldwide, including Netflix and Red Bull. Let’s look at them in more detail.</p><p><strong>What are the Basics?</strong></p><p>Unless you are the owner of a network of servers like they are, their first huge advantage over rolling your own solution is that they use a distributed global network system to bring a copy of your images closer to your users. It’s also far easier for a CDN to ‘future proof’ your image loading strategy as trends change – doing this on your own requires maintenance, tracking browser support for emerging formats &amp; following the image compression community.</p><p>Second, each service has a tiered pricing plan, with Cloudinary offering a <a href="http://cloudinary.com/pricing">free level</a> and imgix pricing their standard level inexpensively, relative to their high-volume premium plan. Imgix offers a free <a href="https://www.imgix.com/pricing">trial</a> with a credit towards services, so it almost amounts to the same thing as a free level.</p><p>Third, API access is provided by both services. Developers can access the CDN programmatically and automate their processing. Client libraries, framework plugins, and API documentation are also available, with some features restricted to higher paid levels.</p><p><strong>Let’s Get to the Image Processing</strong></p><p>For now, let’s limit our discussion to static images. Both Cloudinary and Imgix offer a range of image manipulation methods, and both support primary functions such as compression, resizing, cropping and thumbnail creation in their standard and free plans.<figure><picture><source data-srcset="images/book-images/Modern-Image36-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/Modern-Image36-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/Modern-Image36-large.jpg"><img class="lazyload" data-src="images/book-images/Modern-Image36-large.jpg" alt="cloudinary media library"><noscript><img src="images/book-images/Modern-Image36-large.jpg"></noscript></picture><figcaption>Cloudinary Media Library: By default Cloudinary encodes <a href="http://cloudinary.com/blog/progressive_jpegs_and_green_martians">non-Progressive JPEGs</a>. To opt-in to generating them, check the ‘Progressive’ option in ‘More options’ or pass the ‘fl_progressive’ flag.</figcaption></figure></p><p>Cloudinary lists <a href="http://cloudinary.com/documentation/image_transformations">seven broad image transformation</a> categories, with a total of 48 subcategories within them. Imgix advertises over <a href="https://docs.imgix.com/apis/url?_ga=2.52377449.1538976134.1501179780-2118608066.1501179780">100 image processing operations</a>.</p><p><strong>What Happens by Default?</strong><ul><li>Cloudinary performs the following optimizations by default:</li><li><a href="https://twitter.com/etportis/status/891529495336722432">Encodes JPEGs using MozJPEG</a> (opted against Guetzli as a default)</li><li>Strips all associated metadata from the transformed image file (the original image is left untouched). To override this behavior and deliver a transformed image with its metadata intact, add the keep_iptc flag.</li><li>Can generate WebP, GIF, JPEG, and JPEG-XR formats with automatic quality. To override the default adjustments, set the quality parameter in your transformation.</li><li>Runs <a href="http://cloudinary.com/documentation/image_optimization#default_optimizations">optimization</a> algorithms to minimize the file size with minimal impact to visual quality when generating images in the PNG, JPEG or GIF format.</li></ul></p><p>Imgix has no default optimizations such as Cloudinary has. It does have a settable default image quality. For imgix, auto parameters help you automate your baseline optimization level across your image catalog.</p><p>Currently, it has <a href="https://docs.imgix.com/apis/url/auto">four different methods</a>:<ul><li>Compression</li><li>Visual enhancement</li><li>File format conversion</li><li>Redeye removal</li></ul></p><p>Imgix supports the following image formats: JPEG, JPEG2000, PNG, GIF, Animated GIF, TIFF, BMP, ICNS, ICO, PDF, PCT, PSD, AI</p><p>Cloudinary supports the following image formats: JPEG, JPEG 2000, JPEG XR, PNG, GIF, Animated GIF, WebP, Animated WebP,BMPs, TIFF, ICOs, PDF, EPS, PSD, SVG, AI, DjVu, FLIF, TARGA.</p><p><strong>What About Performance?</strong></p><p>CDN delivery performance is mostly about <a href="https://docs.google.com/a/chromium.org/viewer?a=v&amp;pid=sites&amp;srcid=Y2hyb21pdW0ub3JnfGRldnxneDoxMzcyOWI1N2I4YzI3NzE2">latency</a> and speed.</p><p>Latency always increases somewhat for completely uncached images. But once an image is cached and distributed among the network servers, the fact that a global CDN can find the shortest hop to the user, added to the byte savings of a properly-processed image, almost always mitigates latency issues when compared to poorly processed images or solitary servers trying to reach across the planet.</p><p>Both services use fast and wide CDN. This configuration reduces latency and increases download speed. Download speed affects page load time, and this is one of the most important metrics for both user experience and conversion.</p><p><strong>So How Do They Compare?</strong></p><p>Cloudinary has <a href="http://cloudinary.com/customers">160K customers</a> including Netflix, eBay and Dropbox. Imgix doesn’t report how many customers it has, but it is smaller than Cloudinary. Even so, imgix’s base includes heavyweight image users such as Kickstarter, Exposure, unsplash, and Eventbrite.</p><p>There are so many uncontrolled variables in image manipulation that a head-to-head performance comparison between the two services is difficult. So much depends on how much you need to process the image — which takes a variable amount of time — and what size and resolution are required for the final output, which affects speed and download time. Cost may ultimately be the most important factor for you.</p><p>CDNs cost money. An image heavy site with a lot of traffic could cost hundreds of US dollars a month in CDN fees. There is a certain level of prerequisite knowledge and programming skill required to get the most out of these services. If you are not doing anything too fancy, you’re probably not going to have any trouble.</p><p>But if you’re not comfortable working with image processing tools or APIs, then you are looking at a bit of a learning curve. In order to accommodate the CDN server locations, you will need to change some URLs in your local links. Do the right due diligence :)</p><p><strong>Conclusion</strong></p><p>If you are currently serving your own images or planning to, perhaps you should give a CDN some consideration.</p><h2 id="-a-id-caching-image-assets-href-caching-image-assets-caching-image-assets-a-"><a id="caching-image-assets" href="#caching-image-assets">Caching image assets</a></h2><p>Resources can specify a caching policy using <a href="https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching#cache-control">HTTP cache headers</a>. Specifically, <code>Cache-Control</code> can define who can cache responses and for how long</p><p>Most of the images you deliver to users are static assets that will<a href="http://kean.github.io/post/image-caching"> not change</a> in the future. The best caching strategy for such assets is aggressive caching.</p><p>When setting your HTTP caching headers, set Cache-Control with a max-age of a year (e.g. <code>Cache-Control:public; max-age=31536000</code>). This type of aggressive caching works well for most types of images, especially those that are long-lived like avatars and image headers.<aside class="note"><b>Note:</b> If you’re serving images using PHP, it can destroy caching due to the default <a href="http://php.net/manual/en/function.session-cache-limiter.php">session_cache_limiter</a> setting. This can be a disaster for image caching and you may want to <a href="https://stackoverflow.com/a/3905468">work around</a> this by setting session_cache_limiter(&#39;public&#39;) which will set public, max-age=. Disabling and setting custom cache-control headers is also fine.</aside></p><h2 id="-a-id-preload-critical-image-assets-href-preload-critical-image-assets-preloading-critical-image-assets-a-"><a id="preload-critical-image-assets" href="#preload-critical-image-assets">Preloading critical image assets</a></h2><p>Critical image assets can be preloaded using <a href="https://www.w3.org/TR/preload/"><code>&lt;link rel=preload&gt;</code></a>.</p><p><code>&lt;link rel=preload&gt;</code> is a declarative fetch, allowing you to force the browser to make a request for a resource without blocking the document’s <code>onload</code> event. It enables increasing the priority of requests for resources that might otherwise not be discovered until later in the document parsing process.</p><p>Images can be preloaded by specifying an <code>as</code> value of <code>image</code>:<pre><code class="lang-html">&lt;link rel=&quot;preload&quot; as=&quot;image&quot; href=&quot;logo.jpg&quot;/&gt;
</code></pre></p><p>Image resources for <code>&lt;img&gt;</code>, <code>&lt;picture&gt;</code>, <code>srcset</code> and SVGs can all take advantage of this optimization.<aside class="note"><b>Note:</b> <code>&lt;link rel=&quot;preload&quot;&gt;</code> is <a href="http://caniuse.com/#search=preload">supported</a> in Chrome and Blink-based browsers like Opera, <a href="https://developer.apple.com/safari/technology-preview/release-notes/">Safari Tech Preview</a> and has been <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1222633">implemented</a> in Firefox.</aside></p><p>Sites like <a href="https://www.usa.philips.com/">Philips</a>, <a href="https://www.flipkart.com/">Flipkart</a> and <a href="https://www.xerox.com/">Xerox</a> use <code>&lt;link rel=preload&gt;</code> to preload their main logo assets (often used early in the document). <a href="https://kayak.com/">Kayak</a> also uses preload to ensure the hero image for their header is loaded as soon as possible.<figure><picture><source data-srcset="images/book-images/preload-philips-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/preload-philips-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/preload-philips-large.jpg"><img class="lazyload" data-src="images/book-images/preload-philips-large.jpg" alt="Philips use link rel=preload to preload their logo image"><noscript><img src="images/book-images/preload-philips-large.jpg"></noscript></picture></figure></p><p><strong>What is the Link preload header?</strong></p><p>A preload link can be specified using either an HTML tag or an <a href="https://www.w3.org/wiki/LinkHeader">HTTP Link header</a>. In either case, a preload link directs the browser to begin loading a resource into the memory cache, indicating that the page expects with high confidence to use the resource and doesn’t want to wait for the preload scanner or the parser to discover it.</p><p>A Link preload header for images would look similar to this:<pre><code>Link: &lt;https://example.com/logo-hires.jpg&gt;; rel=preload; as=image
</code></pre></p><p>When the Financial Times introduced a Link preload header to their site, they shaved <a href="https://twitter.com/wheresrhys/status/843252599902167040">1 second off</a> the time it took to display their masthead image:<figure><picture><source data-srcset="images/book-images/preload-financial-times-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/preload-financial-times-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/preload-financial-times-large.jpg"><img class="lazyload" data-src="images/book-images/preload-financial-times-large.jpg" alt="The FT using preload. Displayed are the WebPageTest before and after traces showing improvements."><noscript><img src="images/book-images/preload-financial-times-large.jpg"></noscript></picture><figcaption>Bottom: with <code>&lt;link rel=preload&gt;</code>, Top: without. Comparison for a Moto G4 over 3G on WebPageTest both <a href="https://www.webpagetest.org/result/170319_Z2_GFR/">before</a> and <a href="https://www.webpagetest.org/result/170319_R8_G4Q/">after</a>.</figcaption></figure></p><p>Similarly, Wikipedia improved time-to-logo performance with the Link preload header as covered in their <a href="https://phabricator.wikimedia.org/phame/post/view/19/improving_time-to-logo_performance_with_preload_links/">case study</a>.</p><p><strong>What caveats should be considered when using this optimization?</strong></p><p>Be very certain that it’s worth preloading image assets as, if they aren’t critical to your user experience, there may be other content on the page worth focusing your efforts on loading earlier instead. By prioritizing image requests, you may end up pushing other resources further down the queue.</p><p>It’s important to avoid using <code>rel=preload</code> to preload image formats without broad browser support (e.g. WebP). It’s also good to avoid using it for responsive images defined in <code>srcset</code> where the retrieved source may vary based on device conditions.</p><p>To learn more about preloading, see <a href="https://medium.com/reloading/preload-prefetch-and-priorities-in-chrome-776165961bbf">Preload, Prefetch and Priorities in Chrome</a> and <a href="https://www.smashingmagazine.com/2016/02/preload-what-is-it-good-for/">Preload: What Is It Good For?</a>.</p><h2 id="-a-id-performance-budgets-href-performance-budgets-web-performance-budgets-for-images-a-"><a id="performance-budgets" href="#performance-budgets">Web Performance Budgets For Images</a></h2><p>A performance budget is a ‘budget’ for web page performance that a team attempts to not exceed. For example, ‘images will not exceed 200KB on any page’ or ‘the user experience must be usable in under 3 seconds’. When a budget isn’t being met, explore why this is and how you get back on target.</p><p>Budgets provide a useful framework for discussing performance with stakeholders. When a design or business decision may impact site performance, consult the budget. They’re a reference for pushing back or rethinking the change when it can harm a site’s user experience.</p><p>I’ve found teams have the best success with performance budgets when monitoring them is automated. Rather than manually inspecting network waterfalls for budget regressions, automation can flag when the budget is crossed. Two such services that are useful for performance budget tracking are <a href="https://calibreapp.com/docs/metrics/budgets">Calibre</a> and <a href="https://speedcurve.com/blog/tag/performance-budgets/">SpeedCurve</a>.</p><p>Once a performance budget for image sizes is defined, SpeedCurve starts monitoring and alerts you if the budget is exceeded:<figure><picture><source data-srcset="images/book-images/F2BCD61B-85C5-4E82-88CF-9E39CB75C9C0-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/F2BCD61B-85C5-4E82-88CF-9E39CB75C9C0-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/F2BCD61B-85C5-4E82-88CF-9E39CB75C9C0-large.jpg"><img class="lazyload small" data-src="images/book-images/F2BCD61B-85C5-4E82-88CF-9E39CB75C9C0-large.jpg" alt="SpeedCurve image size monitoring."><noscript><img src="images/book-images/F2BCD61B-85C5-4E82-88CF-9E39CB75C9C0-large.jpg"></noscript></picture></figure></p><p>Calibre offers a similar feature with support for setting budgets for each device-class you’re targeting. This is useful as your budget for image sizes on desktop over WiFi may vary heavily to your budgets on mobile.<figure><picture><source data-srcset="images/book-images/budgets-small.jpg" media="(max-width: 640px)"><source data-srcset="images/book-images/budgets-medium.jpg" media="(max-width: 1024px)"><source data-srcset="images/book-images/budgets-large.jpg"><img class="lazyload small" data-src="images/book-images/budgets-large.jpg" alt="Calibre supports budgets for image sizes."><noscript><img src="images/book-images/budgets-large.jpg"></noscript>
<script>!function(u){"use strict";var e=function(e,t,n){var r,o=u.document,a=o.createElement("link");if(t)r=t;else{var l=(o.body||o.getElementsByTagName("head")[0]).childNodes;r=l[l.length-1]}var i=o.styleSheets;a.rel="stylesheet",a.href=e,a.media="only x",function e(t){if(o.body)return t();setTimeout(function(){e(t)})}(function(){r.parentNode.insertBefore(a,t?r:r.nextSibling)});var d=function(e){for(var t=a.href,n=i.length;n--;)if(i[n].href===t)return e();setTimeout(function(){d(e)})};function s(){a.addEventListener&&a.removeEventListener("load",s),a.media=n||"all"}return a.addEventListener&&a.addEventListener("load",s),(a.onloadcssdefined=d)(s),a};"undefined"!=typeof exports?exports.loadCSS=e:u.loadCSS=e}("undefined"!=typeof global?global:this),function(r){if(r.loadCSS){var e=loadCSS.relpreload={};if(e.support=function(){try{return r.document.createElement("link").relList.supports("preload")}catch(e){return!1}},e.poly=function(){for(var e=r.document.getElementsByTagName("link"),t=0;t<e.length;t++){var n=e[t];"preload"===n.rel&&"style"===n.getAttribute("as")&&(r.loadCSS(n.href,n,n.getAttribute("media")),n.rel=null)}},!e.support()){e.poly();var t=r.setInterval(e.poly,300);r.addEventListener&&r.addEventListener("load",function(){e.poly(),r.clearInterval(t)}),r.attachEvent&&r.attachEvent("onload",function(){r.clearInterval(t)})}}}(this);</script></picture></figure></p><h2 id="-a-id-closing-recommendations-href-closing-recommendations-closing-recommendations-a-"><a id="closing-recommendations" href="#closing-recommendations">Closing recommendations</a></h2><p>Ultimately, choosing an image optimization strategy will come down to the types of images you’re serving down to your users and what you decide is a reasonable set of evaluation criteria. It might be using SSIM or Butteraugli or, if it’s a small enough set of images, going off of human perception for what makes the most sense.</p><p><strong>Here are my closing recommendations:</strong></p><p>If you <strong>can’t</strong> invest in conditionally serving formats based on browser support:<ul><li>Guetzli + MozJPEG’s jpegtran are good optimizers for JPEG quality &gt; 90.<ul><li>For the web <code>q=90</code> is wastefully high. You can get away with <code>q=80</code>, and on 2× displays even with <code>q=50</code>. Since Guetzli doesn’t go that low, for the web you can MozJPEG.</li><li>Kornel Lesi&#x144;ski recently improved mozjpeg’s cjpeg command to add tiny sRGB profile to help Chrome display natural color on wide-gamut displays</li></ul></li><li>PNG pngquant + advpng has a pretty good speed/compression ratio</li><li>If you <strong>can</strong> conditionally serve (using <code>&lt;picture&gt;</code>, the <a href="https://www.igvita.com/2013/05/01/deploying-webp-via-accept-content-negotiation/">Accept header</a> or <a href="https://scottjehl.github.io/picturefill/">Picturefill</a>):<ul><li>Serve WebP down to browsers that support it<ul><li>Create WebP images from original 100% quality images. Otherwise you’ll be giving browsers that do support it worse-looking images with JPEG distortions <em>and</em> WebP distortions! If you compress uncompressed source images using WebP it’ll have the less visible WebP distortions and can compress better too.</li><li>The default settings the WebP team use of <code>-m 4 -q 75</code> are usually good for most cases where they optimize for speed/ratio.</li><li>WebP also has a special mode for lossless (<code>-m 6 -q 100</code>) which can reduce a file to its smallest size by exploring all parameter combinations. It’s an order of magnitude slower but is worth it for static assets.</li></ul></li><li>As a fallback, serve Guetzli/MozJPEG compressed sources to other browsers</li></ul></li></ul></p><p>Happy compressing!<aside class="note"><b>Note:</b> For more practical guidance on how to optimize images, I heavily recommend <a href="https://www.manning.com/books/web-performance-in-action">Web Performance in Action</a> by Jeremy Wagner. <a href="http://shop.oreilly.com/product/0636920039730.do">High Performance Images</a> is also filled with excellent, nuanced advice on this topic.</aside></p><h2 id="-a-id-trivia-href-trivia-trivia-a-"><a id="trivia" href="#trivia">Trivia</a></h2><ul><li><a href="https://jpeg.org/jpegxt/">JPEG XT</a> defines extensions to the 1992 JPEG specification. For extensions to have pixel-perfect rendering on-top of old JPEG, the specification had to clarify the old 1992 spec and <a href="https://libjpeg-turbo.org/">libjpeg-turbo</a> was chosen as its reference implementation (based on popularity).</li><li><a href="https://github.com/google/pik">PIK</a> is a new image codec worth keeping an eye on. It’s compatible with JPEG, has a more efficient color-space and utilizes similar benefits found in Guetzli. It decodes at 2/3 the speed of JPEG and offers 54% more file savings than libjpeg does. It is both faster to decode and compress than Guetzli-ified JPEGs. A <a href="https://encode.ru/threads/2814-Psychovisual-analysis-on-modern-lossy-image-codecs">study</a> on psychovisual similarity of modern image codes showed PIK was less than half the size of alternatives. Unfortunately, it’s still early days for the codec and encoding is unusably slow at this time (August, 2017).</li><li><a href="https://www.imagemagick.org/script/index.php">ImageMagick</a> is often recommended for image optimization. This write-up considers it a fine tool, but its output generally requires more optimization and other tools can offer better output. We recommend trying <a href="https://github.com/jcupitt/libvips">libvips</a> instead, however it is lower-level and requires more technical skill to use. ImageMagick has also histortically had <a href="https://imagetragick.com/#moreinfo">noted</a> security vulnerabilities you may want to be aware of.</li><li>Blink (the rendering engine used by Chrome) decodes images off the main thread. Moving the decode work to the compositor thread frees-up the main thread to work on other tasks. We call this deferred decoding. With deferred decoding, the decode work remains on the critical path for presenting a frame to the display, so it can still cause animation jank. The <a href="https://html.spec.whatwg.org/multipage/embedded-content.html#dom-img-decode"><code>img.decode()</code></a> API should help with the jank problem.</li></ul><p class="license">The content of this book is licensed under the Creative Commons <a href="https://creativecommons.org/licenses/by-nc-nd/2.0/">Attribution-NonCommercial-NoDerivs 2.0 Generic (CC BY-NC-ND 2.0)</a> license, and code samples are licensed under the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 License</a>. Copyright Google, 2018.</p><p></p></div><p></p><script src="scripts/main.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-107163293-1', 'auto');
      ga('send', 'pageview');</script></html>